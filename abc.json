[
  {
    "creator": "Dream11 Engineering",
    "title": "How To Convert a Huge Frontend Monolith to a Micro-frontend",
    "link": "https://blog.dream11engineering.com/how-to-convert-a-huge-frontend-monolith-to-a-micro-frontend-d37f47697235?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 20 Apr 2021 10:27:17 GMT",
    "content:encoded": "<p>When starting a new project, most technology teams begin with a single frontend repository. This worked very well for us at Dream11 too, when our tech team was small in the early stages of projects. But as the team grew, maintaining our content management system (CMS) became more and more difficult. With a growing team, making work processes as seamless as possible to deliver high performance was a priority, so we identified problematic bottlenecks and reinvented our way of work when it came to our CMS application.</p><h3>What were the bottlenecks?</h3><p>We foundÂ that:</p><ul><li>Shared code led to too many conflicts.</li><li>Continuous integration and deployment (CI/CD) became cumbersome, overloaded, and inefficient.</li><li>Experimentation, migration, and refactor became difficult.</li></ul><p>These problems were already tackled in an architecture called microservices, vastly used in back-end architecture design. â€˜Microservicesâ€™ mean dividing a monolithic application into smaller, independent services. On similar lines, Thoughtworks defines micro-frontend as <em>â€œAn architectural style where independently deliverable frontend applications are composed into a greaterÂ whole.â€</em></p><h3>What was our resolution?</h3><p>Work targets are easier to achieve when they are divided into smaller milestones. Similarly, we realized that when a product expands rapidly, it is better to divide the team into smaller verticals. Each vertical could then focus on a specific target- just like an e-commerce application, for example, where there could be separate verticals for offers, user personalization, and checkout.</p><p>We adapted a micro-frontend architecture that helped solve multiple problems. It split scary monoliths into smaller independent applications, which could then be tested, deployed, and maintained in a focused scope. This helped independent vertical teams deliver quicker without worrying about technical debts and heavy regressions, thus, amplifying the overall performance of deliveries.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qMiwXeL7YF-lgUPLZSfXxQ.png\" /></figure><p>There are multiple ways to go about implementing a micro-frontend architecture.</p><ul><li>Handle it at the infrastructure layerâ€Šâ€”â€ŠConfigure your infrastructure to serve multiple applications under the sameÂ domain.</li><li>Handle it at the component levelâ€Šâ€”â€ŠDeploy your components on delivery networks such as <a href=\"https://bit.dev/\">Bit</a>, and use them over the network or burn them into the package during buildÂ time</li><li>Handle it at the run timeâ€Šâ€”â€ŠYou have a controller script/app running on the browser which selects which script/app toÂ load.</li></ul><p>We identified that we could logically separate our flows easily based on routes and thus after evaluating the options, handling at the run time was the right fit for us. It enabled us to haveÂ -</p><ul><li>A micro control on switching the context of applications based on businessÂ logic.</li><li>Authentication, authorization, and layouts were implemented once.</li></ul><h3>High Level Architecture</h3><p>Relating to a microservice analogy, we had multiple independent codebases. In order to serve them as one web application, we implemented a container application that stitched together all the relevant codebases.</p><p>The container application conceptually should know when to switch applications and then bootstrap them into the current runtime. How to decide, can be based on business logic or your application. For our use case, the business flows had very less intersections, so we went with loading different applications simply based onÂ routes.</p><p>The crucial part was bootstrapping individual applications at runtime. Every web application had an entry point where the root component resided. The container application needed access to this entry point component. The child applications made the root component available to the container by hosting it in a globalÂ context.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*RZx9GSyiC9Qrqtl2\" /></figure><p>But how did we implement the architecture? For this, we had to answer two questions for our container -</p><ul><li><strong>When to render a child application?â€Š</strong>â€”â€ŠWe grouped our applications under the router param. For exampleâ€Šâ€”â€Š/App-A/* would serve all routes underÂ App-A.</li><li><strong>How to render a child application?</strong>â€Šâ€”â€ŠIn all our applications, we made the assets and their URLs available using a JSON configuration, along with the entry script to execute to launch a child application.</li></ul><p>Though we used React for all our rendering needs and webpack for bundling, the concept could be applied to any other frontend frameworks.</p><h3>Low LevelÂ Design</h3><p>Now letâ€™s get our hands dirty with some implementation details. Letâ€™s set this up in our local systems. In the above architecture, there are two types of application:</p><ol><li><strong>Container Application</strong></li></ol><p>As mentioned earlier, this application needs to know when and how the child application loads. To achieve this, it needsÂ -</p><ul><li>To load a specific applicationâ€™s bundles based onÂ routes</li><li>To forward asset requests to the specific child application.</li></ul><p>2. <strong>Child Applications</strong></p><ul><li>These applications are hosted independently on different domains</li><li>The child applications need to make it possible for the container to load them This involves adding scripts to mount methods on the window object on the entry point of an application.</li></ul><p>Consider that three applications were running on the following ports on the localÂ system:</p><ol><li><a href=\"http://localhost:2000/\">http://localhost:2000</a>â€Šâ€”â€ŠContainer Application</li><li><a href=\"http://localhost:2000/\">http://localhost:3000</a>â€Šâ€”â€ŠApp-A</li><li><a href=\"http://localhost:2000/\">http://localhost:4000</a>â€Šâ€”â€ŠApp-B</li></ol><p>In order to bootstrap a child application, the container needed to load all the assets required for the child application to start working on the browser. The information about such assets needed to be provided by each child application. Webpack, being our go-to bundler, and <a href=\"https://www.npmjs.com/package/webpack-assets-manifest\">webpack-asset-manifest</a> plugin used with webpack, produced a manifest file containing assets information asÂ follows:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*qn6lvVjmZUxtl4yv\" /></figure><p>Letâ€™s see how the container application structure works. The container application code comprised mainly of threeÂ files.</p><ol><li><strong>MicroFrontend.jsx</strong></li></ol><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/fcb98d86d025f824f24e35528c935a17/href\">https://medium.com/media/fcb98d86d025f824f24e35528c935a17/href</a></iframe><p>The above code is very much similar to what is recommended by <a href=\"https://camjackson.net/\">Cam Jackson</a>. As the name suggests, the purpose of this code was to load the application based on the host and <em>name </em>provided. It made an ajax request for asset-manifest.json and created a script element in the dom. Once the script was loaded, there were two functions added by the child application into its code, i.e renderAppA, and unmountAppA.</p><p>2.<strong> ContainerRoute.jsx</strong></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/71393e501a5a24e755f87111e6f3cdde/href\">https://medium.com/media/71393e501a5a24e755f87111e6f3cdde/href</a></iframe><p>The above code consists of routing information for child application. <em>/App-A/* </em>forwarded all routes to the â€™sÂ router.</p><p>3.<strong> server.js</strong></p><p>The child application may have dynamic chunking implemented in it. When a child application was served from a Container Application, all requests for chunks were to go to the same URL, but instead, they were forwarded to the child applicationâ€™s URL i.e /App-A/Â . To solve this problem, we found and forwarded requests to a specific server. Below is the code that wasÂ used.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/a8049bbd6678fb10ce92f60929b5d3bc/href\">https://medium.com/media/a8049bbd6678fb10ce92f60929b5d3bc/href</a></iframe><p>In this code, we forwarded allÂ .js file requests consisting of App-A<em> </em>in itâ€™s URL, toÂ <em>App-A</em>.</p><p>Along with these three parts, the container app may consist of some common screens. We ended up creating a lightweight login and home page, which were shared by all child applications.</p><p>Now letâ€™s discuss how we addressed the second part of the implementation, that is, children applications. We will take an example of one child app created with create-react-app.</p><p>Following were the changes needed in this application</p><ol><li><strong>index.jsxâ€Šâ€”â€Šattach bootstrapping functions to mount and unmount application</strong></li></ol><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/902b84814036a1dd4f41c57d74415dc4/href\">https://medium.com/media/902b84814036a1dd4f41c57d74415dc4/href</a></iframe><p>In this code, we created two functions for mounting and unmounting child applications into a container application.</p><p>2.<strong> server.jsâ€Šâ€”â€Šfor allowing cross-origin requestÂ based</strong></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/eeb09a4ddb1fc7454ec01b07437216ee/href\">https://medium.com/media/eeb09a4ddb1fc7454ec01b07437216ee/href</a></iframe><p>We allowed cross-domain requests coming from container applications. For security purposes, a list of domains was maintained so that other domain excluding the list could be easilyÂ stopped</p><p>Thatâ€™s it! We are all set to convert any existing application into the micro frontend.</p><h3>The Pros</h3><ul><li>No tech debt was carried: Since all applications were isolated, any breaking change in one wouldnâ€™t affect other applications</li><li>Room for experimentation: One could try different applications stackedÂ together</li><li>Concentrated QA Effort: In the case of regression testing, there was less area toÂ cover.</li></ul><h3>The cons</h3><ul><li>There was a slight impact on the development experience as we had to run multiple applications</li><li>There was less code sharing between applications since the stack of each application was different</li></ul><p>Overall, the benefits of micro-frontend implementation were remarkable and for us, they outweigh the drawbacks.</p><p>Written and implemented by <a href=\"https://www.linkedin.com/in/nishant-dongare-6386bb86/\">Nishant Dongre</a> and <a href=\"https://www.linkedin.com/in/yugal-bagul-22018798/\">YugalÂ Bagul</a></p><h3>References:</h3><ul><li><a href=\"https://martinfowler.com/articles/micro-frontends.html\">Micro Frontends</a> by <a href=\"https://camjackson.net/\">CamÂ Jackson</a></li><li><a href=\"https://medium.com/better-programming/5-steps-to-turn-a-random-react-application-into-a-micro-frontend-946718c147e7\">5 Steps to Turn a Random React Application Into a Micro Front-End</a> by <a href=\"https://jenniferfubook.medium.com/\">JenniferÂ Fu</a></li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d37f47697235\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/how-to-convert-a-huge-frontend-monolith-to-a-micro-frontend-d37f47697235\">How To Convert a Huge Frontend Monolith to a Micro-frontend</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/d37f47697235",
    "categories": ["react", "micro-frontends", "dream11"],
    "isoDate": "2021-04-20T10:27:17.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Enhancing Cloud Security With Real-Time S3 Alerts at Dream11",
    "link": "https://blog.dream11engineering.com/enhancing-cloud-security-with-real-time-s3-alerts-at-dream11-fac99079fbf4?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 19 Apr 2021 06:50:35 GMT",
    "content:encoded": "<p><strong><em>â€” By </em></strong><a href=\"https://www.linkedin.com/in/renin-williams-infosec/\"><strong><em>Renin Williams</em></strong></a><strong><em>, </em></strong><a href=\"https://www.linkedin.com/in/harsimran033/\"><strong><em>Harsimran</em></strong></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*U3punXGR2SU7PRUz\" /></figure><p>At Dream11, keeping our usersâ€™ data safe and secure is of primary importance. Users trust us with their information and we go above and beyond to provide the best possible online fantasy sports experience to them while they enjoy their favourite sports. Since our teams deal with a lot of user-centric and company-related data, every member of the team takes utmost effort and care to keep the data secure. Let us discuss how we made this possible with S3Â alerts.</p><p><strong>The Backgroundâ€Šâ€”â€ŠWhere It AllÂ Began</strong></p><p>At Dream11, we use S3 buckets. They are an integral part of our cloud computing processes and data storage which are accessed by all tech and non-tech teams with appropriate permissions. But what are S3Â buckets?</p><p>S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. S3 buckets are public cloud storage resources that are similar to file folders and store objects consisting of data and its descriptive metadata. The buckets can be made either private or public, depending on the usage and requirement.</p><p><strong>The Issue</strong>:</p><p>When the access control lists (ACL) and the bucket policy are not configured properly, one can knowingly or unknowingly, land up making a bucket (and its contents) public, that is open to all on the internet. Such a misconfiguration can lead to data loss and/or breach of everything that is under the terminology of confidentiality, integrity and availability (CIA).They can go unnoticed if the necessary checks, balances, alerts and alarms are not in place. Auto-remediation can be a step further in drawing the hard-line for enhanced cloud governance.</p><p><strong>Our solution</strong>:</p><p>We believe that in order to solve any issue, it is very important to get visibility and insights into whatâ€™s happening. But how can this be achieved?</p><p>By timely detecting what, when, where, how and by whom the changes were made. In other words, by setting up event based triggers, alarms and alert for real-time notifications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JXsdFbd-WTukHn3P\" /></figure><p>As soon as someone makes a public bucket or changes the existing policies to make a bucket or its content public, based on this event, a lambda can be triggered. This lambda can scan for the change(s), captures them and sends the team a real-time alert on their communication channel(s).</p><p>The team gets the alerts and investigates to understand the situation in hand so the necessary steps may be taken.These can range from simply making the bucket private again to even purging theÂ content.</p><p>Further, the bucket policies can be made tougher as required so the bucket owner or team may be cautioned.</p><p>Hereâ€™s how we tried different solutions:</p><ol><li><strong>We tried open source / tool-based solutions</strong></li></ol><p>We initially referred to a few open-source solutions and monitoring tools. We also used a couple of ready-made scripts from the internet to solve this issue, but it had its limitations and werenâ€™t in line with our automation-first principle..</p><p>We then used the <strong>AWS S3 Access Analyser</strong>, a service offered by AWS. However, this was a manual trigger and had to be run every time while identifying public buckets. There was a lack of real-time and instant visibility and alerts, and so, we had to take manual human driven action, which could be missed anytime. Hence, this approach too didnâ€™t meet our automation-first requirement.</p><p><strong>2. We developed a solutionÂ in-house</strong></p><p><strong>Our first approach:</strong></p><p>Using the python library, Boto developed by AWS, we wrote automation on top of the AWS S3 Access Analyser, to get an automated alert on Email/IM. This didnâ€™t work as required because it relied only on the bucket itself and missed monitoring the objects in theÂ bucket.</p><p><strong>Our second approachâ€Šâ€”â€ŠEUREKA!:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*XsYTq-CZz8Kwq_iI\" /></figure><p>We followed a three-step process:</p><ul><li>Picked up S3 events from cloudtrail.</li><li>Using these Cloud trail events, we set up AWS cloudwatch rules.</li><li>These cloud watch rules in-turn triggers a customÂ Lambda.</li></ul><p>Our custom lambda function does <strong>a lot of conditional checks</strong> on the data received from cloudwatch and based upon the Rules defined, it triggers an alarm for any security anomaly. The alarms are set for custom notifications on our different channels.</p><p>With this solution we were able toÂ detect:</p><ul><li>All public bucket/objects created and/or who is changing the bucket/object permissions, in real-time.</li><li>Actionable notifications were being sent to our communication channelâ€Šâ€”â€Šand on-time action was beingÂ taken.</li></ul><p>We achieved real-time alerts on our S3Â buckets.</p><p>We have made many such automations that will be published soon. So, keep watching this space forÂ more!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fac99079fbf4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/enhancing-cloud-security-with-real-time-s3-alerts-at-dream11-fac99079fbf4\">Enhancing Cloud Security With Real-Time S3 Alerts at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/fac99079fbf4",
    "categories": ["security"],
    "isoDate": "2021-04-19T06:50:35.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "To Scale In Or Scale Out? Hereâ€™s How We Scale at Dream11",
    "link": "https://blog.dream11engineering.com/to-scale-in-or-scale-out-heres-how-we-scale-at-dream11-f88ef5e71cbc?source=rss-52c59dd57d8a------2",
    "pubDate": "Fri, 02 Apr 2021 11:44:14 GMT",
    "content:encoded": "<p>â€” <em>By </em><a href=\"https://www.linkedin.com/in/qamar-ali-shaikh/\"><em>Qamar Ali Shaikh - CoreInfra</em></a><em>Â , </em><a href=\"https://www.linkedin.com/in/ritesh-sharma-a4681592/\"><em>Ritesh Sharma -Â SRE</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*q8aBtIDKpSda70zc\" /></figure><p>For the 100 million Dream11 users, the thrill and excitement of playing fantasy sports on our platform is unparalleled. They enjoy creating their own teams and competing with fellow fans and friends! However, from a backend perspective, there are various challenges we face in terms of variation in traffic and engagement on Dream11 majorly before the match start time. To ensure the application runs smoothly at critical times when user traffic is high, as a team, we came up with a scalable and customisable solution. And so, we were able to run multiple contests simultaneously and efficiently process millions of user requests per second without compromising their experience in playing fantasyÂ sports.</p><p>How did we manage such variation in traffic that drastically fluctuates in short intervals on Dream11 platformÂ ? But before we answer that, letâ€™s have a look at the applicationâ€™s architecture for better understanding.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*To5HL0bM6BGXoLqS6cBLbA.jpeg\" /><figcaption>Dream11 Architecture</figcaption></figure><h3>The architecture supports:</h3><ul><li><strong>A user base of more than a 100Â Million</strong></li><li><strong>User concurrency of over 5.5Â Million</strong></li><li><strong>Over a 100 Million requests per minute (RPM) at edgeÂ services</strong></li><li><strong>More than 30K+ of compute resources to support peak IPLÂ traffic</strong></li><li><strong>More than a 100+ microservices runningÂ parallel</strong></li></ul><h3>Functional Challenges</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hUcor6wPvv-L2R7ClQC47g.png\" /><figcaption>Traffic Surge Events DuringÂ Matches</figcaption></figure><p>The behavioural trend of Dream11 users is a very <strong>spiky</strong> one, (as seen in the Traffic Surge diagram above). This means that the number of users is dynamic and surges frequently, especially when they rush to create/edit fantasy teams and join contests. This also depends on multiple factors such as the popularity of the sport or match and the toss timeline (in the case of cricket matches). Such tsunamis of user traffic on the application can be divided in three main phases, namely, â€˜before the matchâ€™, â€˜during the matchâ€™ and â€˜after the matchâ€™ (including when the winners are declared).</p><p>Once users open the Dream11 application, their <strong>journey</strong> usually oscillates between the home page, tour, team and contest pages. Hence, the load on the application shifts and accordingly, the edge layers, its dependent services and microservices have to be scaled in orÂ out.</p><p>Interestingly, the <strong>user concurrency</strong> may rise or drop in between events or after the matches, and predicting it based on the growth of users per year, can be challenging.</p><p>Let us also consider the <strong>uncontrolled variables</strong> which generate spikes on the platform before, during and after matches. TheseÂ are,</p><ul><li>User interest depends on popularity of the matches and players in real life, which affects the volume ofÂ RPM.</li><li>Match-specific events such as the toss, squad announcement, team creation by the user post squad announcement, and mid-hour events like fall of wickets, hitting of sixes, hat-tricks, breakout events and other unpredictable factors such asÂ rain.</li></ul><p>The tsunami traffic represented by the vertical spikes in the graph above, brings tremendous volatility and load on the infrastructure.</p><h3>Challenges inÂ Scaling</h3><h4>Auto Scaling (why it wonâ€™tÂ work)</h4><p>Autoscaling in terms of infrastructure, has several limitations. Its <strong>provisioning time </strong>is not fast enough to support the compute requirements for users during key events! A flood of users during tsunami traffic needs short provisioning time to keep up with the spike, and it may not be suitable to have a build time and make usersÂ wait.</p><ul><li><strong>Spot availability</strong> of nodes is limited and highly competitiveâ€Šâ€”â€Šespecially at keyÂ hours!</li><li><strong>Step scaling</strong> may not work at this point either, as it is limited to a certain number of nodes, if Dream11â€™s scale is to be considered</li><li><strong>Rebalancing</strong> or rearranging the number of nodes across availability zones (AZ) based on the availability of resources, may further add provisioning cost with respect toÂ time.</li></ul><h4><strong>Limitations of classic and application load balancers (CLB/ALB)</strong></h4><ul><li><strong>Creating load balancers shard based on throughput</strong>, as there is a limit on the number of requests generated on the load balancer. For higher throughput based on user concurrency, there is a need to create shards and manage them as per service routings.</li><li><strong>Pre-warming</strong> of ELBs must be conducted in order to handle the sudden surge ofÂ traffic.</li></ul><h4>Limitations of Cloud Control Plane (Cloud Provider)</h4><ul><li>Additionally, there are limits to the features on our cloud provider too. Application Programming Interface (API ) call rates are limited as per businesses, and this needs to be considered while allocating resources</li><li>Console operations are heavy due to the number of resources provisioned.</li><li>Operational overheads to scale out to 100+ microservices.</li></ul><h3>Solution: Predictive - Proactive Scaling</h3><h4>Our Homegrown Concurrency Prediction Model</h4><p>Our data science team at Dream11 has developed a model for predicting user concurrency using XGboost mode after trying multiple models with 100â€™s of features, to predict the hourly concurrency on the Dream11 platform.</p><p>We first run every matchâ€™s metadata through a linear model which gives the tier of the match. A match tier is an indicator variable for how popular the match willÂ be.</p><p>Match tiers are then categorised (prioritised by high concurrency or those most in demand) based on their past concurrency of similarÂ matches.</p><p>The model then iterates multiple features to predict the concurrency for the particular match. These can be features of each hour such as number of matches by tier in that hour (and in the hours around it), active users in previous hours/days, average transaction sizes etc. All this data goes through a normalisation which can take care of Dream11â€™s exponential growth.</p><p>To top it all, we need a suitable cost-sensitive loss function with no option for under-prediction. In all, we have over 200 variables and more data artistry than data science, making the XGBoost model work with very limited hyper parameter tuning.</p><p>As our data science team believes, <strong>â€œError Analysis &gt;&gt;&gt;&gt; Hyperparameter Tuningâ€.</strong></p><h4>Performance Testing &amp;Â Gamedays</h4><p>Based on the prediction model which provides concurrency estimates, the performance team holds â€˜Game Daysâ€™ to benchmark infrastructure capacity along with factoring trends based on pastÂ matches.</p><p>The performance testing framework used isÂ <a href=\"https://about.dream11.in/blog/2020/12/finding-order-in-chaos-how-we-automated-performance-testing-with-torque\"><strong>Torque</strong></a></p><p>The infrastructure provisioning frameworkÂ : <strong>Playground (Watch this space for more onÂ this)</strong></p><p>Using Playground to provision Infrastructure and <a href=\"https://about.dream11.in/blog/2020/12/finding-order-in-chaos-how-we-automated-performance-testing-with-torque\">Torque</a> to run performance tests, the performance team certifies the following improvements for business functionality based on user concurrency predictions.</p><p><strong>Performance metrics &amp; validationsÂ :</strong></p><ul><li>Defining the application latencyâ€Šâ€”â€Šthe acceptable latency to serve theÂ business</li><li>Identifying individual serviceÂ capacity</li><li>Benchmarking compute and network throughput</li><li>Identifying failure and saturation points of the applications</li><li>Generating sudden spikes and surges to identify impact on backend infrastructure and identifying cascading effects in the architecture</li><li>Test Infrastructure boundaries w.r.t Compute, Network, API Limits and Operations.</li></ul><h4>Deployment optimisations to reduce provisioning time</h4><ul><li>Fully baked Amazon machine images ( AMI ) for deployments with application artefact for fasterÂ scaling</li><li>Provisioning multiple compute instance types across Availability Zones (diversified), reducing capacity challenges</li><li>Capacity Optimised allocation strategy for spot instances</li><li>Cost Optimisation ensuring 100% resources are running onÂ spot</li><li>Notifications to failure in case of spot unavailability and enable on demand provisioning.</li><li>Weighted domain name system (DNS) records to support ELBÂ shards.</li></ul><h4>Scaling for the key hours usingÂ Scaler.</h4><ul><li>The DevOps and SRE team at Dream11 have orchestrated a platform â€˜Scalerâ€™ which helps in ProActive scalingÂ ,per the concurrency prediction and performance benchmarks.</li><li>Based on the performance tests with respect to the predicted concurrency and trend, the system is fed with different slabs of user concurrency and respective infrastructure to provision across microservices before the match, during the match, and after theÂ match.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*BRQbgXcDZiD4Hfo9\" /><figcaption>Scaler Flow</figcaption></figure><p><strong>Technology and Cloud ServicesÂ Used</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*n36Qctm2DQZUkQkp\" /><figcaption>Tools and Technology Used</figcaption></figure><h3>ğŸ–Results Achieved!</h3><p>ğŸš€ <strong>Improved quality with lower risk: Scaler</strong> helps the Dream11 DevOps/SRE team to manage Scale-In and Scale-Out operations much better, while making our process much more efficient, considering the sheer volume of matches hosted on the platform.</p><p>ğŸš€ <strong>Faster service:</strong> It streamlined daily operational processes and infrastructural tasks, what previously took hours to complete is now achieved inÂ minutes.</p><p>ğŸš€ <strong>Increased flexibility: Scaler</strong> helps us save a significant amount of time, as scaling operations are now based on schedules of matches. This increases operational efficiency and enables the DevOps/SRE team to focus on making engineering improvements.</p><p>ğŸš€ <strong>Lower infrastructure cost:</strong> As the scaling operations are scheduled based on different slabs and tiers, overall capacity of infrastructure can be provisioned based on the events of matches. This reduces the monthly infrastructure cost byÂ 50%.</p><p>ğŸš€ <strong>Distinctive insights: </strong>Analytics based on the scaling events and user trends provide better feedback to the machine learning model. This makes it possible to predict organic and inorganic growth patterns of infrastructure and users, which in turn helps us predict the provisioning requirements for theÂ future.</p><h3>Future Scope ofÂ Work</h3><p>As we mature the architecture, we are looking at predictive and scheduled scaling for <strong>containers</strong> and <strong>data stores. </strong>We are also looking to optimise our infrastructure cost and to scale out realtime basis the spike we see on the Dream11 platform. To achieve this we are looking for talented engineers excited in solving infrastructure problems at scale and delivering a great product to Dream11Â users.</p><p>If you think solving above challenges interests you? We at Dream11 have open positions for those who are SMEâ€™s in their domain. <a href=\"https://jobs.lever.co/dreamsports\">ApplyÂ Now</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f88ef5e71cbc\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/to-scale-in-or-scale-out-heres-how-we-scale-at-dream11-f88ef5e71cbc\">To Scale In Or Scale Out? Hereâ€™s How We Scale at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/f88ef5e71cbc",
    "categories": ["scale", "devops", "data-science", "aws", "microservices"],
    "isoDate": "2021-04-02T11:44:14.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "How does Dream11 serve millions of Personalised Contests in a match",
    "link": "https://blog.dream11engineering.com/how-does-dream11-serve-millions-of-personalised-contests-in-a-match-cbb00463b7da?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 12 Jan 2021 09:42:54 GMT",
    "content:encoded": "<p>â€” <em>By</em><a href=\"https://www.linkedin.com/in/srijan--gupta\"><em> Srijan Gupta</em></a>,<a href=\"https://www.linkedin.com/in/anuj3918/\"> <em>AnujÂ Gupta</em></a></p><p>At a time when we are collectively homebound, thanks to the pandemic, the Dream11 Indian Premier League (IPL) 2020 came like a breath of fresh air for cricket fans everywhere. Not only did the <strong>Dream11 IPL 2020</strong> fill an otherwise gaping void in the realm of sports this year, but it also kept our passion for cricket, a thread that binds us together, burning as brightly asÂ ever.</p><p>Besides watching the matches, sports fans participated in exciting fantasy sports contests on the Dream11 app and showcased their skill and knowledge of the sport! Fans can create their own team of real-life players from upcoming matches, score points based on their on-field performance and compete with other fans. Whatâ€™s more, as Title Sponsors of the Dream11 IPL, we were committed to providing fans with a secure and seamless experience.</p><figure><img alt=\"Contest sections\" src=\"https://cdn-images-1.medium.com/max/320/0*sL3arOWdYa7ckpZD\" /><figcaption>Contest sections</figcaption></figure><p>Different types of <strong>contests allow users to compete and win big, thereby increasing their engagement.</strong> Creating customised or personalised contests are a prime component for best user experience. With hundreds of highly personalised contests running on the platform simultaneously, meeting every userâ€™s requirements, it is inevitable to encounter several challenges during the entire process. To keep these contests running smoothly, we play an entirely different ball game at the backend. So what are these, and how do we manage to address each ofÂ them?</p><h3>Business Requirements for Creating Personalized Contests forÂ Users</h3><p>Contests are the centre of attraction for every user, which motivates us to manifest contests without any hiccups and ensure a smooth-sailing experience. For that, we need to meet a set of business requirements that will enable us to do exactly this. Each contest is different from the others in terms of variables like prize money, contest entry amount, number of winners and total participants. To pique our usersâ€™ interest and provide contest options best suited for them, the contest page is divided into Sections like â€˜<em>Mega Contests,</em>â€™ â€˜<em>Head to Head Contests</em>,â€™ â€˜<em>Contest for Champions,</em>â€™ and â€˜<em>Top Picks For You</em>,â€™ to name aÂ few.</p><p>Several events are generated every minute as users interact with the application. We wanted to leverage the data collected from these events to create <strong>User Cohorts</strong>. A user cohort is a logical grouping of users based on their activity, playing pattern andÂ history.</p><p>For receiving a maximum response, we apply target campaigns to compartmentalise users and serve relevant contest sections to them based on their cohorts. These cohorts keep changing in real-time basis user activity and events. <strong>Cohorts are mapped to relevant Contest Sections thus allowing users to see exclusive sections.</strong></p><p>The Product and Data team at Dream11 leverage the following featuresÂ too:</p><ul><li><strong>Personalise contests based on user attributes</strong></li><li><strong>Rules based exclusive contests</strong></li><li><strong>Run A/B tests on contests and sections to figure out which variant works theÂ best</strong></li></ul><h3><strong>How we maintain user-cohorts</strong></h3><p>Self serve admin panel is provided to write/maintain rules to create cohorts and map relevant contest sections to thatÂ cohort.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Gv9sQ-cCmgRZxWBT\" /><figcaption>User cohortsÂ flow</figcaption></figure><p>Once the cohort is activated, a query to data-lake is fired to bootstrap users who pass the cohort rules into the cohorting engine. Then this data is streamed to kafka as <em>cohort_in</em> event to be consumed by user-cohort service.</p><p>The users keep falling in and out of the cohorts based on real-time events as being processed by the cohorting engine.</p><h3><strong>How we serve fast-filling contests at Dream11Â Scale</strong></h3><p>In a typical IPL match, there is a lot of incoming traffic. Once the contests are filled to their optimum capacity, they are dynamically replaced from backend with new contests in real time to keep the inventory refreshed.</p><p>After the toss of every match, we typically see a high surge in contest joins per second. This leads to a high rate of contests creation, adding upto millions of contests in a singleÂ match.</p><p>The starting point is contest creation which is done by our AI based Contest Generation Engine (CGE). CGE calls Contest-Generation service which generates different kinds of contests in contest datastore i.e.Â VoltDB.</p><p>From there, we stream these generated contests to <strong>Spark Streaming</strong> where it continuously writes new contests to our contest cache store <strong>Aerospike </strong>and expires the filled contests. This needs to be done in real-time as users need to see updated contestÂ data.</p><p>We chose Aerospike since it is a distributed, scalable database with intelligent re-balancing and data migration. But like any other distributed system it has <strong>Eventual Consistency</strong> to compensate for High Availability which leads to another important problem to beÂ solved:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/382/0*n3tuV9ss4XdnID9A\" /><figcaption>Contests spots gettingÂ filled</figcaption></figure><p><strong><em>Hot-key scenario</em></strong><em>:</em> As so many users are playing together, our typical user concurrency is around 5 million post-toss. Since everyone is trying to play the same match, they will see a similar list of contests. Clients need to reflect the number of spots left at sub-second level. Serving 5M users from a single aerospike node is impossible as it will create a <strong>HOT-KEY</strong>. So, we need to replicate this data to other nodes and read from Master and SlavesÂ both.</p><p><strong><em>Eventual consistency trust issue:</em></strong> If we replicate (replication factor &gt;= 10) then eventual consistency may cause users to see the â€˜<em>Number of spots leftâ€™</em> increasing (different data because of eventual consistency between nodes) which can lead to big trust issues among users. The number of spots can either decrease or remain constant. Under no circumstances should they increase.</p><h3><strong>Solution:</strong></h3><p>We started doing <strong>key seeding </strong>for all active contests in aerospike while keeping the replication factor of each key as 2. Key seeding basically means creating multiple copies of the same data and writing it to cache. We configured our spark streaming to write 10 duplicate copies of <em>match_contests</em> key by appending _1, _2,â€¦,_10 to the keys and hashed each userId so a particular user will always read from a particular copy. This made sure that a user always sees â€˜<em>Number of spots leftâ€™</em> as strictly decreasing even if spark fails to write a fewÂ copies.</p><pre>int keySeed = userId % (no. of copies);<br>Map contestData = getFromAerospike(â€˜match_contests_â€™ + keySeed);</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ug_XOF5CbGd_UjAa\" /><figcaption>Active contestsÂ flow</figcaption></figure><p><strong>Request response cycle:</strong> On landing on the contests page, for each user a request is made to contest microservice which does the following:</p><ol><li>Get userâ€™s cohorts from <em>user-cohorts</em> microservice.</li><li>Prepare a list of contest sections based on the userâ€™sÂ cohorts.</li><li>Read filtered contest sections from aerospike to get active contests.</li><li>Asks <em>promotions</em> microservice for any running promotions.</li><li>Sends back the response.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*RQ1_WCG9GazefJRF\" /><figcaption>Contests servingÂ flow</figcaption></figure><p>This system has been a great success to generate contests and give a personalised touch to them through cohorting. At Dream11, we vouch for the quality and output of work to make fantasy sports an unforgettable experience forÂ users.</p><p>Interested in building similar systems for serving 100M &amp; growing users, reach out to usÂ <a href=\"https://jobs.lever.co/dreamsports/d3814c13-dcb8-4169-be73-7bc55ad978e7\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cbb00463b7da\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/how-does-dream11-serve-millions-of-personalised-contests-in-a-match-cbb00463b7da\">How does Dream11 serve millions of Personalised Contests in a match</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/cbb00463b7da",
    "categories": ["distributed-systems", "scale", "dream11", "sharding"],
    "isoDate": "2021-01-12T09:42:54.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Data Highwayâ€Šâ€”â€ŠDream11â€™s Inhouse Analytics Platformâ€Šâ€”â€ŠThe Burden and Benefits",
    "link": "https://blog.dream11engineering.com/data-highway-dream11s-inhouse-analytics-platform-the-burden-and-benefits-90b8777d282?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 07 Jan 2020 11:35:20 GMT",
    "content:encoded": "<h3>Data Highwayâ€Šâ€”â€ŠDream11â€™s Inhouse Analytics Platformâ€Šâ€”â€ŠThe Burden andÂ Benefits</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*Xw8WVdTzCjfYT8cfVLsv-Q.png\" /><figcaption>Scale atÂ Dream11</figcaption></figure><p>With a whopping 40 million RPM, 2.5 million user concurrency, it is all about capturing and crunching the data of the 3 billion daily events that we receive, owing to data collection of ~4.5 TB perÂ day.</p><p>In the current data driven world, how fair is to expect a company to count on a single data tool to fulfill business and data requirements!!?<br> I would say, it is clearly not possible. With each tool fighting to have its own unique functionality, we tend to lean towards outsourcing of different tools to satisfy our data and business needs. Fair Enough!<br> <br> However, if you see your data growing manifold every year, at some point down the line, it gets difficult for some of these tools to handle the humongous inflow of data. Our idea of Inhouse Analytics has its genesis from this fundamental need to have more control over our own data, to have more flexibility as per internal requirements and to play with the data as per our discretion, without relying much upon outsourced tools.</p><p>Donâ€™t let this implementation give you an illusion that it is a snap! <br> The implementation of Inhouse analytics comes with its own set of advantages and challenges!<br> <br> We initially used Segment.io to capture our interaction data and Redshift to store our transactional data. Inhouse analytics involves gathering the user interaction eventsâ€™ data along with transactional data at one place. This would enable us to easily map user actions to a specific transaction, revise definition of metrics as per our business, build user interaction-based audience profiles to send out a promotion, and the list goesÂ on!</p><h3><strong>Dream11 Data Platform Architecture</strong></h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T5BXJKxZ4AWxkoaAwRCTVw.png\" /></figure><h3><strong>What does Inhouse Analytics have in store forÂ us?</strong></h3><p><strong>#1 A Centralised Tool For Reporting</strong></p><p>With all possible data at one place, we are eventually phasing out multiple tools that we have been using to report different metrics and working towards sticking to one tool. For instance, we used Google Analytics for analysing interaction data, Segment.io to store user interaction data, Fabric to check user concurrency, Looker to build models and report transactional data.<br> <br> With Inhouse Analytics in place, we are now consuming data from Amazon Redshift/Athena/Druid and building models on Looker to report user concurrency, to track user journeys and to build interaction events basedÂ funnels.</p><p><strong>#2 User Mapping Across Multiple Platforms</strong></p><p>Currently, there are limitations with every tool that are available in the market with respect to identifying and mapping a specific userâ€™s action across multiple platform (Web/App). Although, Google Analytics has the functionality of mapping users across platforms, it doesnâ€™t function accurately for user journey funnels. This use case is specifically important if your company is a parent to other companies or if your business is present on both web, android andÂ iOS.</p><p><strong>#3 DataÂ Sampling</strong></p><p>Due to the high inflow of data, reporting tools like Google Analytics or Amplitude sample user data and show the approximate overall counts or drop offs in a funnel This might be misleading while taking product based decisions.<br> <br>Using Inhouse analytics avoids data sampling and lets us get an accurate picture of the current productÂ state.</p><p><strong>#4 Accelerated Speed of Reporting</strong></p><p>Reporting tools take significant amount of time to load data in order to generate unsampled reports or even in case of basic reporting when the data volume is huge. This issue could be resolved when we start aggregating data as per our requirements using inhouse analytics.</p><p><strong>#5 Mapping of User Interaction and Transaction Data</strong></p><p>For businesses which involve transactions that are not invoked due to user interaction, it gets slightly difficult to analyse the impact of any interaction on the transaction. We have been using Google Analytics to map this.. However, it couldnâ€™t solve the use case to map server events data with client events data. Inhouse Analytics would solve this problem easily as we have entire data at one place, with unique identifiers for each interaction.</p><p><strong>#6 DataÂ Security</strong></p><p>With growing competition and data privacy issues it has become extremely important to securely store the data and avoid sharing granular data with different tool. Data security is one of the biggest advantages to have everything inÂ house.</p><p><strong>#7 ReduceÂ Cost</strong></p><p>With increasing volume of events, the storage and processing cost charged for each tool is shooting up. In order to limit the costs and build a sturdy system, in-house analytics seem to be the bestÂ way.</p><p><strong>#8 Data Integration with OtherÂ Sources</strong></p><p>Inhouse analytics framework gives us the flexibility to integrate with other sources by selectively sharing required data. For instance, we can build an audience in-house based on the data that we have and feed it to Clevertap to send push notifications to thoseÂ users.</p><h3><strong>Implementation Challenges</strong></h3><p><strong>#1 Unique Identifier for AÂ User</strong></p><p>We assign a unique identifier to a logged in user but what about the visitors who have no unique ID assigned to them. Google Analytics used to solve this use case by assigning a unique ID to every user (visitors/logged in) which made it easy to identify. <br> In order to solve this use case, we started mapping users based on advertiser ID/cookie ID/device ID of aÂ user.</p><p><strong>#2 Retrospective Mapping of a Logged In User to his Non-Logged InÂ State</strong></p><p>If a user comes to the app but is not logged in, and after some time, if he/she logs in, tools like Google Analytics/Amplitude do the retrospective mapping of logged in user to the visitor state and indicate that both are same. <br> In case of Inhouse analytics, since we get multiple entries for the same user with different IDs, it was difficult to map directly.<br> We have to work towards setting up this mapping based on device ID/advertiser ID</p><p><strong>#3 Real Time Chronological User Interaction Path</strong></p><p>This path of every user is helpful for the Customer Support team to resolve any real time queries for refundsÂ etc.</p><p>We get realtime stream of events on Kafka. We have setup a pipeline to push this data from Kafka to Elastic search. Using Kibana dashboards, we can easily visualize the user journeys and get real-time user interaction paths to debug theÂ issue.</p><p><strong>#4 Demographics and LocationÂ Data</strong></p><p>Google Analytics captures both demographics and user location data based on the user repository it has. However, we wouldnâ€™t be able to track that information as there is no source to get it. This still remains a challenge and we are exploring ways to solveÂ it.</p><p><strong>#5 Static Table Schema Not Adapting to Additional NewÂ Fields</strong></p><p>AWS Glue Crawler has helped us build a catalog and daily scheduled has ensured it has kept the catalog up-to-date forÂ users.</p><p><strong>#6 Dynamic FunnelÂ Creation</strong></p><p>Google Analytics provides effective dynamic funnel creation capabilities and when we moved this to Inhouse, it was fun and challenge in itselfâ€Šâ€”â€Šit is an interesting journey about how we provided capabilities to build funnels dynamically. We plan to cover that in the nextÂ blog</p><p>Implemented By <a href=\"https://www.linkedin.com/in/pradip-thoke-04b4b910/\">Pradip Thoke</a>, <a href=\"https://www.linkedin.com/in/salmandhariwala/\">Salman Dhariwala</a>, <a href=\"https://www.linkedin.com/in/naincy-suman-2012/\">Naincy Suman</a>, <a href=\"https://www.linkedin.com/in/vikasrgite/\">Vikas Gite</a>, <a href=\"https://www.linkedin.com/in/lavanya-pulijala/\">Lavanya Pulijala</a>, <a href=\"https://www.linkedin.com/in/ruturajbhokre/\">Ruturaj Bhokre</a>, <a href=\"https://www.linkedin.com/in/dhanraj-gaikwad-88246723/\">DhanrajÂ Gaikwad</a></p><p>Written By <a href=\"https://www.linkedin.com/in/lavanya-pulijala/\">LavanyaÂ Pulijala</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=90b8777d282\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/data-highway-dream11s-inhouse-analytics-platform-the-burden-and-benefits-90b8777d282\">Data Highwayâ€Šâ€”â€ŠDream11â€™s Inhouse Analytics Platformâ€Šâ€”â€ŠThe Burden and Benefits</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/90b8777d282",
    "categories": [
      "sports-data",
      "data-engineering",
      "inhouse-analytics",
      "dream11",
      "dream11-data"
    ],
    "isoDate": "2020-01-07T11:35:20.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Building Scalable Real Time Analytics, Alerting and Anomaly Detection Architecture at Dream11",
    "link": "https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33?source=rss-52c59dd57d8a------2",
    "pubDate": "Thu, 19 Sep 2019 12:09:24 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/pradip-thoke-04b4b910/\">Pradip Thoke</a>, <a href=\"https://www.linkedin.com/in/salmandhariwala/\">Salman Dhariwala</a></p><p><strong>Introduction</strong></p><p>Building batch data analytics platform traditionally is not a new trend. While the industry is moving towards agile and shorter development cycles, scope of building data platform is no more limited to batch processing. Businesses aim for real time updates on-the-go. No one wants to know something that has broken after anÂ hour.</p><p>Many of us must have seen an application use RDBMS OLTP directly and run SQL statements to do all these. In case you are wondering- <em>is this a good solution</em>? The answer is- itÂ depends.</p><p>Using OLTP transactional systems to run your real time analytics might be enough for your use case. However as project requirements grow and more advanced features are neededâ€Šâ€”â€Šfor example, enabling synonyms, joint analytics or doing lookupsâ€Šâ€”â€Šyour relational database might not beÂ enough.</p><p>Our journey at Dream11 was no different. It started with fulfilling requirements from traditional way and as complexity started biting us in terms of volume and latency, we moved to more powerful, distributed real-time engines.</p><p><strong>Data volume atÂ Dream11</strong></p><ul><li>Deals with 3+ TB of data perÂ day</li><li>10M+ transactions perÂ day</li><li>2.8 M concurrent users</li><li>Billions of clickstream events perÂ day</li></ul><p><strong>Why Dream11 decided to implement real-time pipeline</strong></p><p>1) At the core of Dream11 Engineering, we use AWS Aurora as an OLTP system. These systems are very efficient for OLTP load but are not meant for OLAP kind of workload. We canâ€™t perform aggregation or OLAP type of queries on these transactional systems.</p><p>2) To solve the above problem obvious solution is to run your OLAP queries on your data warehouse. In our case, it was AWS Redshift. We have ETL pipelines in place to load data from transactional system to our warehouse. These ETL pipelines run every hour thus real-time analytics is not possible on warehouse</p><p><strong>What we wanted toÂ achieve</strong></p><p>We wanted to perform real-time analytics on the data, which is residing in the transactional system, in our case AWS Aurora database.<br>Some of the use cases are as follow:Â -</p><ol><li>Know the real-time rate of contestÂ joins</li><li>Know the real-time aggregated status of paymentÂ gateways</li><li>Identify real-time anomalies eg: PG going down, unusual traffic on theÂ system</li><li>Realtime aggregated view of outcome of marketing campaigns</li><li>How customers are using discount coupons once promotion goesÂ live</li><li>Realtime alerting once Mega contest is aboveÂ 90%</li></ol><h4>Architecture overview</h4><p>So, letâ€™s explore How Dream11 has implemented the pipeline. To understand the architecture better, we will divide the pipeline into various stagesÂ -</p><ol><li>Pull data from various transaction systems on a common bus in real-time</li><li>Perform data enrichment</li><li>Push data into a real-time data store for a real-time analytics. This allows data visualization, rule-based anomaly detection andÂ alerting</li><li>Operations &amp; Monitoring</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XYFledX05nJOja7kRilbdg.png\" /></figure><h4>Moving data into Apache Kafka with the KafkaÂ Connect</h4><p>We have used Kafka connect to pull data from our transactional system to Kafka. For this, we have primarily used Debezium &amp; JDBCÂ source.</p><p>Kafka Connect, an open source component of Kafka, is a framework for connecting Kafka with external systems such as databases, key-value stores, search indexes, and file systems. Using Kafka Connect, you can use existing connector implementations for common data sources and sinks to move data into and out of Kafka. Kafka Connect is focused on streaming data to and from Kafka, making it simpler for you to write high quality, reliable, and high-performance connector plugins. It also enables the framework to make guarantees that are difficult to achieve using other frameworks. Kafka Connect is an integral component of an ETL pipeline when combined with Kafka and a stream processing framework. Kafka Connect can run either as a standalone process for running jobs on a single machine (e.g., log collection), or as a distributed, scalable, fault tolerant service supporting an entire organisation. This allows it to scale down to development, testing, and small production deployments with a low barrier to entry and low operational overhead, and to scale up to support a large organisationâ€™s data pipeline.</p><p><strong>Source Connector</strong><br>A source connector ingests entire databases and streams table updates to Kafka topics. It can also collect metrics from all your application servers into Kafka topics, making the data available for stream processing with lowÂ latency.</p><p><strong>Sink Connector</strong><br>A sink connector delivers data from Kafka topics into secondary indexes such as Elastic search or batch systems such as Hadoop for offline analysis.</p><p><strong>The main benefits of using Kafka ConnectÂ are:</strong></p><p><strong>Data Centric Pipeline</strong>â€Šâ€”â€Šuse meaningful data abstractions to pull or push data to Kafka.<br><strong>Flexibility and Scalability</strong>â€Šâ€”â€Šrun with streaming and batch-oriented systems on a single node or scaled to an organization-wide service.<br><strong>Reusability and Extensibility</strong>â€Šâ€”â€Šleverage existing connectors or extend them to tailor to your needs and lower time to production.</p><p><a href=\"https://github.com/debezium/debezium\"><strong>Debezium</strong></a> is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things goÂ wrong.</p><p>The<strong> JDBC source connector</strong> allows you to import data from any relational database with a JDBC driver into Apache Kafka topics. By using JDBC, this connector can support a wide variety of databases without requiring custom code for each one. Data is loaded by periodically executing a SQL query and creating an output record for each row in the result set. The database is monitored for new or deleted tables and adapts automatically. When copying data from a table, the connector can load only new or modified rows by specifying which columns should be used to detect new or modifiedÂ data.</p><p>Learnings<strong>:</strong></p><ol><li>Debezium works on Binlog, it adds overhead on the database which will eventually impact the performance of the database.</li><li>JDBC source cannot detect deletes and intermediate updates.</li><li>JDBC can miss long running transactions</li></ol><h4>Enriching datasets withÂ KSQL</h4><p>Confluent KSQL is the streaming SQL engine that enables real-time data processing against Apache Kafka It provides an easy-to-use, yet powerful interactive SQL interface for stream processing on Kafka, without the need to write code in a programming language such as Java or Python. KSQL is scalable, elastic, fault-tolerant, and it supports a wide range of streaming operations, including data filtering, transformations, aggregations, joins, windowing, and sessionization.</p><p>We have used KSQL for various use cases mainly Stream join, Data enrichment, Filtering, Aggregation</p><p>Learnings:<br> 1) Although KSQL is a very powerful tool it is in early stages. We have observed some data loss issue.<br> 2) KSQL file mode deployment is preferred over CLI mode. CLI mode some time causes duplicate recordsÂ issue</p><h4>Indexing documents with Elastic Search Connector</h4><p>The Elasticsearch connector allows moving data from Apache Kafka to Elasticsearch. It writes data from a topic in Kafka to an index in Elasticsearch and all data for a topic have the sameÂ type.</p><p>Learnings:</p><ol><li>We have created weekly indexes in Elasticsearch for easy archival of older data. We can use topic index map property to create weeklyÂ indices</li><li>Supports upsert/append mode</li><li>Schema mappings needs to be handledÂ properly</li></ol><h4>Setting up Alert rules for anomaly detections and real time dashboards</h4><p><a href=\"https://github.com/Yelp/elastalert\">ElastAlert</a> is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch</p><p>Several rule types with common monitoring paradigms are included with ElastAlert:</p><p><em>Match where there are at least X events in Y timeâ€ (frequency type)<br>Match when the rate of events increases or decreasesâ€ (spike type)<br>Match when there are less than X events in Y timeâ€ (flatline type)<br>Match when a certain field matches a blacklist/whitelistâ€ (blacklist and whitelist type)<br>Match on any event matching a given filterâ€ (any type)<br>Match when a field has two different values within some timeâ€ (change type)<br>Match when a never before seen term appears in a fieldâ€ (new term type)<br>Match when the number of unique values for a field is above or below a threshold (cardinality type)</em></p><h4>Latency Monitoring:</h4><p>IPL is our peak time. During this time, we have observed end to end maximum latency of less than 2 minutes. New Relic monitoring dashboards in place with required alerts with thresholds in place for attention if something goes wrong withÂ pipeline</p><h4>Wrapping itÂ up</h4><p>With this architecture we were able to achieve ingestion rate of ~40k events/sec with an end to end latency of &lt; 50 secs. This has helped our operational team take real time decisions on adding more contests if required. Product team is using this to know current inflow, user activities and decide on strategy on giving promotions and making user journeyâ€™s more interactive. Marketing team is using this to know their campaign effectiveness in real time and make adjustments on theÂ fly.</p><p><strong>Whatâ€™s next? DataÂ Highway</strong></p><p>We would be sharing our journey of implementing clickstream real time event processing in-house framework at large scale. Thatâ€™s Data Highway. Stay tunedÂ :)</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e20edec91d33\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33\">Building Scalable Real Time Analytics, Alerting and Anomaly Detection Architecture at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/e20edec91d33",
    "categories": ["dream11", "big-data", "real-time-analytics"],
    "isoDate": "2019-09-19T12:09:24.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "DATA is the New OIL: Mine it Well to Tap your Customers in Real-Time",
    "link": "https://blog.dream11engineering.com/data-is-the-new-oil-mine-it-well-to-tap-your-customers-in-real-time-e01803c321b0?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 25 Mar 2019 08:56:55 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/neha-sood-60719013/\">NehaÂ Sood</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1022/0*S6muBLTOUrmpHfKF\" /></figure><p>Data is the new oilâ€¦only if you can reach meaningful insights out of it. Getting the most relevant insights in the fastest possible way, makes a business stand apart from the crowd. In other words, reliability and speed are the two key metrics when it comes to assessing the quality of insights. However, with growth in user base and resultant data, supporting deep analytics at a large scale becomes a challenge.</p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, our analytics use-cases range from real-time customer targeting based on their current actions, detecting behavioural anomalies, powering journey nudges basis usersâ€™ action stage, enabling the business to react withoutÂ delay.</p><p>Because of our humongous scale, i.e., over 50 million registered users and 1 billion events per day, creating pertinent architecture is challenging, not to mention we are growing 4xÂ YoY.</p><p>One such use-case was to identify fraudulent users and their transactions in near-real-time. The objective was to enable instant withdrawals for all our users. This required running a decision engine on their entire history of transactions in near-realtime, which the current architecture could notÂ support.</p><h3>Run to theÂ hills!</h3><p>Our transactional data currently resides in <a href=\"https://aws.amazon.com/rds/aurora/\">Aurora</a>, which could not scale for this use-case. With a microservices based architecture, all the required data is also distributed across different clusters. Therefore, running joint analytics was not possible, asÂ well.</p><p>Below is the broad architecture to kick start theÂ project:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*o1aDmvO41EWVGwA2\" /></figure><p>While looking for a target NoSql datastore, we focussed on the below prerequisites:</p><ul><li>Low response time for data atÂ scale</li><li>Highly available and futuristic, in line with our growthÂ target</li><li>Ability to handle request spikes &amp; high write volumes in peak seasons like IPL and World CupÂ etc.</li><li>Horizontally scalable</li></ul><p>For the above requirements, <a href=\"http://cassandra.apache.org/\">Cassandra</a> seemed a good fit. Modelling your data right (as per the use-cases) is the key to make it work. Data enrichment and joins have to be a part of the streaming layer.</p><h3>The Pipeline from Aurora (AWS RDS) toÂ Kafka</h3><p>With our <a href=\"https://medium.com/dream11-tech-blog/a-journey-to-insightful-data-286756496fbd\">previous pipeline</a>, we already had the data flowing from Aurora toÂ Kafka.</p><p>However, from our experience with maintaining Confluent Connectors, we knew they were not the right fit for a near-real-time use-case.</p><h4>Adios confluent connectors (why)</h4><ul><li>Long-running transactions cause data sourcingÂ delays.</li></ul><p>For sourcing data from Aurora to Kafka through Confluent connectors, we used â€œTimestamp + Incrementing modeâ€. To ensure that a transaction with an earlier timestamp completes, before proceeding the counter, some delay needs to be added with â€œtimestamp.delay.interval.msâ€, accounting for the maximum transaction duration perÂ table.</p><ul><li>Continuously updating records are not sourced, until their updations pause for aÂ while.</li></ul><p>A record is sourced only when it is in the &lt;LastSourcedOffset&gt; to &lt;Current timestamp-Delay interval&gt; window, with respect to the record timestamp. If the nature of data demands continuous updates, chances are theyâ€™ll keep missing this window until the updates slow down or pause for aÂ while.</p><p>For more details on our usage of Confluent Connectors, refer our last blogÂ <a href=\"https://medium.com/dream11-tech-blog/a-journey-to-insightful-data-286756496fbd\"><strong><em>here</em></strong></a><strong><em>.</em></strong></p><h4><a href=\"https://www.go-fiji.com/bula.html\">Bula</a> Binlog replicators</h4><p>Thinking of CDC on transactional systems and not considering Binlog replicators is a cardinal sin. We ran a comparison across all the known and less-known pieces. Here is what we have toÂ show:</p><p>Comparisons (as ofÂ Mayâ€™18)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*27ljb2FEZ3YGo4Pj\" /></figure><p><strong>Voila!</strong></p><p>Maxwell, Streamsets and Debezium came out as the best suited for our requirements. However, with Maxwell lacking HA and Streamsets only supporting HA in its enterprise version, Debezium was the â€˜chosenâ€™ one. With a performance difference of ~15â€“20% under heavy load, some internal load tests comparing Streamsets community version &amp; Debezium, also favoured the latter. Being based on confluent connectors, it also supported <a href=\"https://docs.confluent.io/current/schema-registry/docs/index.html\">Schema Registry</a> out-of-the-box.</p><p><strong>Gotchas inÂ Debezium</strong></p><ul><li>Beware, Debezium is not a scalable replicator. A single connector thrives on a single node. So you may run into performance bottlenecks. In our load tests, we started seeing lags at a consistent DML of at least 6k DMLs/sec, for more than 5 minutes. So, with consistently increasing and high load, this will be a bottleneck, although it may catch up within a couple of seconds, as soon as the load fluctuates or goesÂ down.</li><li>With shared binlog files, across nodes, mysql master failover can get handled with a small operational overhead. All you need is to restart the Debezium connector, using their HTTPÂ API.</li><li>Use avro data format, rather than Json to save network I/O. With Avro, we witnessed more than 50% spike in the performance.</li><li>To make Debezium suit our scale, we jumped into its open source code and tried out the below optimizations. The first one is running for us in production:</li><li>Skip deserializing delete (or any other) row operations, if they are not relevant to your streaming pipeline.</li><li>Serialize date(Time) fields as long (<a href=\"https://github.com/shyiko/mysql-binlog-connector-java/blob/master/src/main/java/com/github/shyiko/mysql/binlog/event/deserialization/EventDeserializer.java\">https://github.com/shyiko/mysql-binlog-connector-java/blob/master/src/main/java/com/github/shyiko/mysql/binlog/event/deserialization/EventDeserializer.java</a>)</li></ul><h3>Streaming: Kafka to Cassandra</h3><p>Industrial benchmarks favored <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">structured streaming</a> as a micro-batch low latency and high throughput engine. Our POC showed good results too. With easy support for late data arrivals, joining multiple streams, out-of-the-box mechanisms for fault-tolerance and state-management, this looked like the perfect fit. An icing on the cake: the code is easily unit testable.</p><p>Also, we chose EMR for running the processes.</p><h4>Why EMR?</h4><ul><li>AWS managedÂ service</li><li>Supports resource dynamic allocationâ€Šâ€”â€Šyour streaming jobs can automatically handle spikes in data volume and be able to scale up as your dataÂ grows</li><li>Auto-scaling of core and task nodes based on CloudWatch metrics</li><li>Automatic node failureÂ handling</li></ul><h4>Gotchas inÂ EMR</h4><ul><li>Master node stores the fsimage and the following edit logs. EMR does not merge them automatically, which results in disk fullÂ issue.</li><li>As can be concluded from the above point, EMR doesnâ€™t have a secondary name node aka checkpoint node.</li><li>Task nodes do not have any local HDFS, unlike core nodes. So, any tasks running on these nodes will need to fetch the data from remote core nodes, for processing. This will have some performance implications.</li></ul><h3><a href=\"https://www.collinsdictionary.com/dictionary/english/beefeater\">Beefeaters</a> &amp;Â <a href=\"https://www.collinsdictionary.com/dictionary/english/raven\">ravens</a></h3><ul><li>With both Debezium and Spark Streaming, you can expose metrics via JMX. This helps easily integrating with centralized dashboards that can give you a fair end-to-end picture on delays and possible bottlenecks.</li><li>Make sure you also set-up alerts when binlog replicator or streaming latency exceeds the expected threshold.</li></ul><h3>Is my toast done,Â honey?</h3><p>What we have is, an in-house generic Streaming framework, which can read from and write to configurable Kafka and Cassandra clusters, with automatic schema mapping, using Schema Registry. With this framework, apart from the ability to join across streams, supporting dynamic filtering rules and data deduplication, one may even rename/drop/derive new fields from existingÂ ones.</p><p>The pipeline involving multi-stream join, with large join and watermarking windows in the streams, takes max 2 sec end-to-end. While the single stream pipelines, with some filtering rules, column add/drop/rename, works on a delay of less than 100Â ms.</p><p>We have just begun our journey of real-time deep analytics. The next frontier is identifying and alerting anomalies and tracking near-real-time user behaviour patterns.</p><p><strong>Follow us in our journey and keep learning withÂ us.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e01803c321b0\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/data-is-the-new-oil-mine-it-well-to-tap-your-customers-in-real-time-e01803c321b0\">DATA is the New OIL: Mine it Well to Tap your Customers in Real-Time</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/e01803c321b0",
    "categories": ["big-data", "dream11"],
    "isoDate": "2019-03-25T08:56:55.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "ElasticSearch @Dream11",
    "link": "https://blog.dream11engineering.com/elasticsearch-dream11-30328d913cd5?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 25 Feb 2019 15:55:53 GMT",
    "content:encoded": "<p><strong>ElasticSearch @</strong><a href=\"https://www.dream11.com/\"><strong>Dream11</strong></a></p><p>Scale to serve over 500,000+ events per second, 5 TB+ ingestion capacity and provide logging, search for micro services, security analytics along with anomaly detection</p><p>By <a href=\"https://www.linkedin.com/in/sanjeev-jaiswal-3ab9259/\">Sanjeev Jaiswal</a>, <a href=\"https://www.linkedin.com/in/astrobounce\">ArvindÂ Sharma</a></p><p><em>How DevOps team at </em><a href=\"https://www.dream11.com/\"><em>Dream11</em></a><em> built â€˜Elasticsearch as a Serviceâ€™ for our microservicesÂ , security, data services logging and analytics needs in the face of high events frequency and data ingestion requirements</em></p><p><strong>Introduction:</strong></p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, multiple microservices and applications are using Elasticsearch as a core infrastructure engine to solve problems like centralized logging, search capability in applications, storing security related WAF logs and anomaly detection etc. All these use cases involve querying/searching, structured as well as unstructured data with specific keywords at very high events per second with large amounts of logs ingestion per hour/day.</p><p><strong>Why Elasticsearch is in ObviousÂ Choice?:</strong></p><p><a href=\"https://www.dream11.com/\">Dream11</a> was using a SAAS based solution for its centralized loggingÂ needs.</p><p>Rsyslog was forwarding the logs to the SAAS based provider for our services. However, during our peak game cycle amid popular sporting events, the logging service failed to deliver in time. Events per second -EPS was so high that, the logs to the SAAS provider lagged behind and did not show up in dashboards even after days or weeks. This defeated the very purpose of having the need for real-time logging system as a service. Engineering, DevOps and QA teams were severely hampered and not able to debug issues in time and had to revert to additional monitoring tools or actually having to login to systems for debugging. The SAAS provider acknowledged that due to our scale, they would need to re-architect their solution. This could take couple of precious months! Hence we decided to build in-house Elastic search for our peak scale, data ingestion and internal usecases. This was the rise of â€˜ElasticSearch as a Serviceâ€™.</p><p>AWS Elastic search was ruled out after initial analysis due to our high throughput events per second rate(EPS rate) of 200K/second and logging rate of 1 TB to 1.5 TB per day during peak gaming season andÂ matches.</p><p><strong>ElasticSearch and FluentD evaluation:</strong></p><p>Before implementing the solution, we evaluated different log forwarders and aggregators Log stash, Rsyslog and Fluentd. We appreciated Fluentdâ€™s integration, performance, support and plugins as compared to others.Therefore, we decided to go ahead with an internal â€˜Elastic FluentD Kibanaâ€™ stack(s) which can be managed within a â€˜reasonableâ€™ time and DevOps resources.</p><p>Terraform and Ansible were used in building the ElasticSearch stack as aÂ service.</p><p><strong>Centralised Logging usingÂ EFK</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*pchfwm1Oye-fV7PV\" /></figure><p><strong>Architectural componentsÂ :</strong> EFK stack at <a href=\"https://www.dream11.com/\">Dream11</a> is divided into the following components</p><ul><li><strong>Cluster Discovery using discovery-ec2 pluginâ€Šâ€”â€Š</strong>Cluster discovery of Elasticsearch is done using ec2 discovery plugin and unique tags of theÂ cluster.</li><li><strong>Log Forwarders ( td-agent )</strong>â€Šâ€”â€Štd-agent is installed on each and every server and is just used for log shipping to log aggregators</li><li><strong>Log Aggregators ( td-agent )â€Š</strong>â€”â€ŠLog aggregators process the stream of data coming from log forwarders based on different tagsÂ . Data is enriched and sent to Elasticsearch coordinator nodes for indexingÂ .</li><li><strong>Elasticsearch coordinators and Kibanaâ€Š</strong>â€”â€ŠElasticsearch coordinator nodes are Elasticsearch node types which act as cluster load balancer. These also have Kibana nodes installed, Kibana just takes single Elasticsearch URL till 6.4.0. This HA of Kibana is maintained using HA of coordinator nodes.</li><li><strong>Elasticsearch mastersâ€Š</strong>â€”â€ŠThese nodes are cluster managers whose work is just to manageÂ cluster</li><li><strong>Elasticsearch Hot Nodesâ€Šâ€”â€Š</strong>These are Elasticsearch data nodes with node attribute property set to hot. These nodes have more CPU and RAM nvme SSD for higher write throughputÂ .</li><li><strong>Elasticsearch Warm Nodesâ€Šâ€”â€Š</strong>These are again Elasticsearch data nodes with node attribute property set to warm. These nodes stores all indices from previous day to retention periodÂ .</li><li><strong>AWS S3â€Šâ€”â€Š</strong>S3 is used for archival of indicesÂ . All indices are kept as per our index retention policies on hot and warm nodes.When the retention period expires they are moved to S3 and deleted from theÂ cluster.</li><li><strong>Curatorâ€Šâ€”â€Š</strong>Curator is used for keeping indices retention policiesÂ , archivalÂ , movement of index from to warmÂ , increasing replicaÂ count.</li><li><strong>Elastalert Kibana Plugin: </strong>Since we are using open source version of Elasticsearch, alerting on Elasticsearch data doesnâ€™t comes out of boxÂ . We have used another open source tool, i.e Elastalert developed by Yelp and its Kibana UI plugin developed by Bitsensor to provide efficient alerting frameworkÂ .</li></ul><p><strong>Hardware Configurations: </strong>Indexing is resource intensive.So choosing hardware at our scale has been quite cumbersome. There had been a lot of load test to finally come to these hardware configuration and numbersÂ .</p><ul><li><strong>Master Nodes</strong>: <strong>3 m5.xlarge</strong> instances used as masters are the one which holds all metadata information and perform all cluster management tasksÂ . To avoid split brain scenario odd number of master is used along with ec2 discovery plugin.<br> discovery.zen.minimum_master_nodes:2</li><li><strong>Hot and Warm Nodes: 5 i3.2xlarge </strong>instances are used as hot nodes with node attributes set to hot or warm depending on the node typeÂ . i3.2xlarge provides high end machines with 1.9 TB nvme disk which outruns the performance of normal EBS backed SSD disks. <strong>10 d2.xlarge</strong> instance are used as warm nodes for index storage upto retention period. d2 instances have 3 instance disks with 2TB throughput HDD eacÂ .</li><li><strong>Client/Coordinator &amp; Kibana Nodes: </strong>These are in ASGâ€™s with <strong>r5.2xlarge </strong>instances which scale according to workload.Each machine is has master, dataÂ , ingest set to false to use it as coordinator nodes. Moreover, every machine is having Kibana installed to get rid of SPOF of KibanaÂ .</li><li><strong>Fluentd Aggregators: 5 c5.2xlarge </strong>are used for aggregators instances which perform enrichment and transformation.</li></ul><p><strong>Index Lifecycle ManagementÂ : </strong>For managing lifecycle of indices curator framework is usedÂ . Curator is another tool written in python which can be used for replica management, snapshot and restore etc. Here a sample action file for curator is used for index management</p><p>actions:</p><p>1:</p><p>action: allocation</p><p>description: â€œApply shard allocation filtering rules to the specified indicesâ€</p><p>options:</p><p>key: box_type</p><p>value: warm</p><p>allocation_type: require</p><p>wait_for_completion: false</p><p>timeout_override:</p><p>continue_if_exception: false</p><p>disable_action: false</p><p>filters:</p><p>- filtertype: pattern</p><p>kind: prefix</p><p>value: logstash-</p><p>- filtertype: age</p><p>source: name</p><p>direction: older</p><p>timestring: â€˜%Y.%m.%dâ€™</p><p>unit: days</p><p>unit_count: 1</p><p>2:</p><p>action: delete_indices</p><p>description: â€œDeleted selectedÂ indicesâ€</p><p>options:</p><p>timeout_override: 300</p><p>continue_if_exception: false</p><p>ignore_empty_list: True</p><p>filters:</p><p>- filtertype: pattern</p><p>kind: prefix</p><p>value: logstash-</p><p>- filtertype: age</p><p>source: name</p><p>direction: older</p><p>timestring: â€˜%Y.%m.%dâ€™</p><p>unit: days</p><p>unit_count: 7</p><p><strong>Security Analytics UsingÂ EFK</strong></p><p>Another use case where we are highly dependent on EFK is performing security analyticsÂ . We use AWS WAF to secure our websites. To make more out of AWS WAF, analysing WAF logs is a goldmineÂ . AWS WAF can stream WAF logs in realtime using Amazon Kinesis Firehose. Firehose has certain limitation for the sinks (storage of logs).It currently provides only 4 options whichÂ are</p><ul><li>AWS Elasticsearch Service</li><li>AWS S3</li><li>Redshift</li><li>Splunk</li></ul><p>As we built our own high efficient highly scalable Elasticsearch cluster, we did some POC to push these logs to our Elasticsearch clusterÂ . For this we utilized AWS S3 and itâ€™s event notification service which pushes metadata event in SQS for every new data put into s3 bucket. Using another Fluentd plugin we created consumers of SQS whose capability was to read from SQS metadata and then finally capture data from s3 bucket and push it to our Elasticsearch clusterÂ .</p><p>Below architecture diagram describes the complete flow forÂ same.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*VJFi6Ou0nlWfafsr\" /></figure><p><strong>KPI Monitoring using Elasticsearch andÂ Kibana</strong></p><p>Another use case for which we are using Elasticsearch is KPI monitoring where KPIâ€™s are being pushed into Kafka. We are using KSQL and Kafka to transform the data. Kafka Connect Elasticsearch connector pushes this data into Elasticsearch.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Ps1djHSfbh0cWLWO\" /></figure><p><strong>Results:</strong></p><p>The entire stack(s) were load-tested and tweaks were made constantly to achieve stability and performance. We finally managed to achieve our peak scale requirements and ingestion rate after few weeks of tweaking and testing.We now use Terraform scripts to setup the Elasticsearch stack(s) for any new services or requirements on demand as a â€˜serviceâ€™, much better than what AWS or any other SAAS provider can provide in terms of cost, stability, performance and resources.</p><p><strong><em>Cost savings has been a huge side effect because we manage our ElasticSearch service with the desired high performance, throughput along with customizations that can be done quickly with minimal resources!</em></strong></p><p><strong>Adoption of our stack companyÂ wide:</strong></p><ol><li>Since the â€˜ElasticSearch as a Serviceâ€™ proved successful, we used it for our Security Analytics needs as well for analysis of WAF logs. Custom queries were written to scan suspicious IP addresses and block them after review on demand. A separate stack was setup for the security analytics and alerting was implemented using â€˜ElastAlertâ€™ for alerting our SecurityÂ team</li><li>Our stack has then been extended for use for Anomaly detection and Contest fill rate by our DataScience Engineering team. Separate Kibana dashboards and visualisations have been tuned for different requirements and performance.</li></ol><p><strong>Metrics:</strong></p><p>Estimated ingestion capacity of our ElasticSearch stack(s) was measured at approx 5 TBâ€Šâ€”â€Š6TB per day, which means capacity to hold approx 150TB to 180 TB data in 30 days! Events per second have been recorded at 500,000 events per second per day maximum as of now and can be scaled further ifÂ needed.</p><p><strong>Further optimisations and futureÂ plans:</strong></p><ol><li>Implementation of Federated ElasticSearch services, so that failure of one EFK stack for a specific use-case does not affect other users of that EFKÂ stack.</li><li>Introducing Kafka as messaging and buffer service. During peak traffic, we have seen that FluentD aggregators can get backlogged and buffers filled very quickly. We plan to introduce Kafka in the architecture while pumping data to FluentD aggregators to absorb any temporary back-pressure problems and store the data on disk. With introduction of Kafka as buffer approach, data can be forwarded at a later point in time to EFK stack without any dataÂ loss.</li><li>Authentication and authorization services using SearchGuard.</li><li>Machine Learning module to analyse logs and train models on our security data for automatic alerting on suspicious trends and activities from our application logs andÂ WAF.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30328d913cd5\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/elasticsearch-dream11-30328d913cd5\">ElasticSearch @Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/30328d913cd5",
    "categories": ["elasticsearch"],
    "isoDate": "2019-02-25T15:55:53.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Dream11 Android Application Architecture",
    "link": "https://blog.dream11engineering.com/dream11-android-application-architecture-303f04b6b5b2?source=rss-52c59dd57d8a------2",
    "pubDate": "Wed, 09 Jan 2019 11:04:14 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/rahul-panchal-2410a034/\">RahulÂ Panchal</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*H6RBEqzLAAdbFh_UXfpTuw.png\" /></figure><p><strong>Designing the mobile application architecture</strong></p><p>Over the years, mobile application developers have experimented with various standard architecture patterns like Model View Controller (MVC), Model View Presenter (MVP), Model View ViewModel (MVVM), and clean architecture et al. These patterns need improvisations to implement it for specific requirements of the mobile app. While designing the architecture, the first step is to identify and state the objectives. Below were the objectives identified byÂ us:</p><ol><li>Single source ofÂ truth.</li><li>The business logic should be suitable for unit testing andÂ reuse.</li><li>Follow <a href=\"https://hackernoon.com/solid-principles-simple-and-easy-explanation-f57d86c47a7f\">SOLID</a> principles to ensure scalability, flexibility and integration of new components in the future. Such scalability and flexibility should come only at a nominal overheadÂ cost.</li><li>It should adapt to the necessary platform APIs and external SDKs (like Facebook, analytics SDKs etc.) without compromising the fundamentals.</li><li>Each and every class of the project must be in-line with the architectural principles. This helps in seamless initialization and any further reference to theÂ class.</li></ol><p>In this blog series, we will discuss the architecture of <a href=\"https://www.dream11.com/\">Dream11</a> Android app, and future improvements that we plan to undertake:</p><p><a href=\"https://www.dream11.com/\"><strong>Dream11</strong></a><strong> Android Architecture</strong></p><p>Our philosophy is to design and maintain a <a href=\"http://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">Clean Architecture</a>. We have segregated it in 4 layers, View, Presenter, Model and Services. We are using RxJava for data streaming, Dependency Injection for object accessing and DataBinding library to update the view and obtain events from the view. It is important to note that we are using data binding, but not the MVVM philosophy, where View Models are the most intelligent entities. For us, View models are just POJO classes which hold data for a view and obtain events fromÂ view.</p><p>Clean Architecture has guidelines for the object dependency (producer-consumer) and the code structure</p><ol><li><strong>Producer-Consumer object dependency <br></strong>Having a clean<strong> </strong>architecture means that the producer should not be dependent on the consumer, as in below diagram, where view layer is consuming the observables from the presenter and the presenter is consuming the observables from model layer and so on. In this case, producer layer objects can be created without the dependency on the consumerÂ layer</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*MVvI9-yRWwKv1fNM\" /></figure><p><strong>2. Code structure </strong>We can segregate mobile application code in 4Â parts.</p><ol><li>Platform specific code (Service Layer, ViewÂ Layer)</li><li>Enterprise specific code (FeatureÂ Layer)</li><li>Application specific code (FeatureÂ Layer)</li><li>Interface/Adapters (Presenter Layer)</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/854/0*36jmtgKzTPX7cFfk\" /></figure><p>We have written enterprise and application specific logic in the same layer to reduce communication between layers and centralise all decision making at oneÂ place.</p><p><strong>View Layer: </strong>It comprises of Activities and Fragments. Each flow has one Activity and multiple Fragments for respective screens. For example, <strong>LoginActivity</strong> manages different screens (fragments) of login flow. This approach provides tremendous flexibility and ease. For instance, whenever we need to discontinue the flow, we can simply close the activity. We also use flow specific activities to trigger prerequisite execution.</p><p><strong>Presenter Layer: </strong>It is an interface/adapter between view layer and the model layer. It has below responsibilities:-</p><ol><li>It creates ViewModels using the data received from the feature layer and exports them to view layer through observables</li><li>It delegates the events of view layer to respective feature layerÂ class</li><li>While mapping models to view models, it also adds view specific logic, ifÂ required</li></ol><p><strong>Feature Layer</strong></p><p>This is the most intelligent layer in our architecture. It contains the enterprise logic and the app logic (deciding the flow of an application). Below are some interesting characteristics of thisÂ layer:</p><ul><li><strong>Feature classes are segregated on the basis of business logic types: </strong>Since data sharing across different app components needs wiring code, we minimized the number of feature classes by using logic type and not screens like <strong>LoginFeature</strong>, <strong>PaymentFeature</strong>. As a result, the logic for all login related screens are handled by <strong>Login Feature</strong>. Similarly, the logic for all payment related screens is handled by <strong>PaymentFeature</strong> class, because same business components can share the data with relativeÂ ease.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/267/0*RYhtJXH3L5QwDrka\" /></figure><p>Feature classes can communicate with each other through Request Message Pool, RMP is a special kind of event bus, we will discuss it in detail in the nextÂ blog</p><ul><li><strong>Own/Monitor the data:</strong> We have a single source of truth for data. But, instead of the single state container, it is fragmented in respective feature classes. For instance, the <strong>PaymentFeature</strong> class takes care of the Payment state of the app, so all the feature classes are a singleton, created using the Factory-Pattern.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/593/0*PrfPmnLGEPZdAx1e\" /></figure><ul><li><strong>Communication:</strong> There is an input and two output flows on the feature layer. One who initiates the action and gets the output as the return of method flow <strong>Observable&lt;FeatureResponse&gt;</strong>. Since our feature classes are also maintaining a single source of truth of data, with every change of data, they push it on <strong>FeatureUpdate</strong> channel. This is subscribed by all the other relatedÂ views.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/284/0*UhEORgNRV0rxonxm\" /></figure><p><strong>Service Layer</strong></p><p>Model Layer should be platform independent. This allows all the platform specific functions like making HTTP calls, a message read permission, log in through Facebook and Google, saving or retrieving values from database to be written at the serviceÂ layer.</p><p>Some Android SDK APIs seeks message read permissions while the others (Facebook, etc.) need an instance of the activity. Clean architecture principles do not permit access to the platform object instance in enterprise or app specific code. Additionally, we cannot put them in the view layer because a producer cannot depend on its consumerÂ objects.</p><p>To solve this problem, we created <strong>UIListener</strong> interface. Here, the object is being passed to the service layer class, which provides current activity and application object to serviceÂ classes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/739/0*oQAD1n8-lpzkuB9e\" /></figure><p><strong>Things to be improved</strong>:</p><ol><li><strong>Java to Kotlin</strong>: As of now, we are using Java. But we have also started Kotlin for some components and features. Data classes of Kotlin are very effective for Layered Architecture since every layer has to ensure that its original data objects cannot be changed by the consumer layers. For this, we need to create a copy of data objects while passing it to consumerÂ layer.</li><li><strong>Removing side effects and separating code in pure and impure functions</strong>: As of now, we are separating code in pure and impure functions manually. But in the future, we would like to create a framework which enforces pure functions and separates impure logic in an impure function. This will make it easy to write testÂ cases.</li><li><strong>REST to Graphql: </strong>As an organisation, we are progressing from REST to Graphql, as it allows enhanced communication between client and server. It has certain advantages like being strictly typed and the best place to write common logic for all the clients. It reduces the complexity of code at the client side, reduces state size and provides view ready data to them. Since it is an abstraction layer between client and server, their architectures can be developed more independently.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=303f04b6b5b2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/dream11-android-application-architecture-303f04b6b5b2\">Dream11 Android Application Architecture</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/303f04b6b5b2",
    "categories": [
      "android",
      "android-app-development",
      "mobile-app-development",
      "mobile-apps",
      "clean-architecture"
    ],
    "isoDate": "2019-01-09T11:04:14.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "A Journey to Insightful Data",
    "link": "https://blog.dream11engineering.com/a-journey-to-insightful-data-286756496fbd?source=rss-52c59dd57d8a------2",
    "pubDate": "Wed, 17 Jan 2018 12:38:35 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/neha-sood-60719013/\">NehaÂ Sood</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*MSlkshBJKjXD3rj7Q4F48Q.jpeg\" /></figure><blockquote>Despite the sense of promise surrounding Big Data, most companies estimate they are only analysing 12% of their data, while 80% of data on average is unstructured and does not lend itself to analysisÂ easily.</blockquote><p>Big Data is much more than simply a matter of sizeâ€Šâ€”â€Šit presents an opportunity to discover key insights and emerging trends in data, makes businesses more agile, board room decisions better informed, and answer questions that have previously been considered unanswerable. With all the hype around big data, insightful data is eventually most important to business.</p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, with a growing user base of over 18 million users &amp; ~3x YoY growth, running deep analytics &amp; building useful insights from the data, started seeing scalability bottlenecks. The need for having a central &amp; scalable data platform became unavoidable. <em>This post is about how we overcame the scalability and architecture challenges.</em></p><p><strong>Challenges faced:</strong></p><p>1.<strong> Evolving/Changing Needs</strong></p><p>The fast pace at which we were growing and the rising deep analytics requirements soon outdated our existing systems. Amazon <a href=\"https://aws.amazon.com/rds/aurora\">Aurora</a> is our choice of backend relational database and all our transactional data resides there. With a rapidly growing, current data volume of a few TBs, running deep data analytics was not possibleÂ anymore.</p><p>2. <strong>Fragmented Analytics</strong></p><p>All our transactional data &amp; user-activities event data is being maintained in 2 distinct platforms. While one is in-house, the other one is a store maintained by a third party tool, Segment. With 2 disconnected data platforms, there is no way to run joint analytics. For instance: we need to run a simple A/B test on our platform wherein we want to experiment with the colour of â€œJoin Contestâ€ button. The success metric is how many of our users end up doing a transaction after they click on the particular button. This needs access to transactional data and clickstream data that are on different and disjointed data platforms.</p><p>3. <strong>Real-time analytics atÂ scale</strong></p><p>The current data pipeline lacked streamlining. There was a huge opportunity here to enable &amp; deliver real-time analytics.</p><p><strong>The execution:</strong></p><p><strong>Step 1: Identifying the DataÂ Lake</strong></p><p>To begin with, we needed to identify a <strong>Data Lake</strong>, where we could keep all type of data from all possible sources, in its raw and native format. This unstructured/semi-structured/structured data could then be refined and sourced into other data-stores based on the use-cases as well as business requirements.</p><p>Primary features we needed from the DataÂ Lake:</p><p><em>â–ª High Durability, scalability, availability &amp; data security at lowÂ cost</em></p><p><em>â–ª Support for heterogeneous data types: structured, unstructured &amp; streaming</em></p><p><em>â–ª Easy to use: Data ingestion to analytics</em></p><p>We narrowed down our choice toÂ <a href=\"http://aws.amazon.com/s3\"><strong>S3</strong></a>.</p><p><strong>Why S3 as DataÂ Lake?</strong></p><p><em>â–ª 11 9s of Durability; 4 9s of availability</em></p><p><em>â–ª High performance, low cost &amp; low maintenance solution</em></p><p><em>â–ª Decouple storage &amp; compute. Both can be scaled independently</em></p><p><em>â–ª Polyglot Analytics: Provision most suitable processing engine/system based on the business requirement</em></p><p>This is the broad platform overview we came upÂ with:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*TfzIkeXhoRG555kSGLg_iA.png\" /></figure><p><strong>Step 2: Identifying the Data Warehouse</strong></p><p>Next, we needed a data-warehouse that would store hot/warm data, with at least theseÂ features</p><p><em>â–ª Easy to scale &amp; highly performant</em></p><p><em>â–ª Supports the existing BI tools we have already invested-in (monetarily &amp; â€œeffort-wiseâ€)</em></p><p><em>â–ª Easy maintenance</em></p><p><a href=\"https://aws.amazon.com/documentation/redshift/\">Redshift</a> worked for us perfectly. Five key performance enablers for Redshift: <a href=\"https://en.wikipedia.org/wiki/Massively_parallel\">MPP</a> engine, <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html\">columnar data storage</a>, <a href=\"https://aws.amazon.com/blogs/aws/data-compression-improvements-in-amazon-redshift/\">data compression</a>, <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/c-query-planning.html\">query optimisation, and compiled code</a>. Columnar storage optimises query costs by increasing the effective storage capacity of the data nodes and improves performance by reducing I/O needed to process SQL requests. Redshiftâ€™s execution engine also transforms each query into compiled code, which then runs in parallel on each computeÂ node.</p><p>Some other side benefits included:</p><ul><li><em>Its SQL</em></li><li><em>Jump-Startâ€Šâ€”â€Šyou can set up a robust and fast data warehouse with no DBA in a fewÂ hours.</em></li><li><em>Already on AWSâ€Šâ€”â€ŠSince our application stack is already on AWS, working with Redshift didnâ€™t involve any bigÂ leaps</em></li><li><em>Automatic updates &amp;Â patching</em></li><li><em>Less maintenance overheads</em></li></ul><p>An important benefit of our choices of Data Lake and Data Warehouse was that they worked really well with each other. With S3, data ingestion in Redshift can be massively parallelised, with each compute node directly working on data load, rather than being bottlenecked on leader nodeâ€™s networkÂ I/O.</p><p>This is the Data Platform we have in place rightÂ now:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*luxRpFmur_oSJICM3uzr0w.png\" /></figure><p>While the user-events data still needs to be handled and pipelined into the system, the transactional data becomes available in the Data Warehouse within 2 hours of occurrence.</p><p>Some important pointers:</p><ol><li>Using Kafka as the communication pipeline gives this architecture additional ability to also do real-time analytics on this data, if/asÂ needed.</li><li>For every structured dataset pushed to Kafka, we also maintain corresponding metadata in the â€œSchema Registryâ€, for easy consumption and also for handling schemaÂ changes.</li><li>In S3, each dataset is placed into difference keyspaces, further partitioned by year-month-date-hour, for data pruning optimizations and easy incremental loads. Data-partitioning rule and granularity is also configurable per data source, ifÂ needed.</li></ol><p><strong>Step 3: Identifying data pipeline buildingÂ blocks</strong></p><p><strong>Aurora to S3: Confluent connectors</strong></p><p>Confluent is a platform that improves Apache Kafka by expanding its integration capabilities, adding tools to optimise and manage Kafka clusters. Confluent Platform makes Kafka easier to build and easier to operate. For sourcing data from our Aurora cluster to S3 via Kafka, we are using <a href=\"https://www.confluent.io/product/connectors/\">Confluent Connectors</a>.</p><p>Why Confluent Connector:</p><ul><li><em>Easy to scale &amp; maintain, with transparent execution</em></li><li><em>Supports Avro</em></li><li><em>Support for JDBC/Kafka/S3</em></li><li><em>No back-pressure on source DB (unlike AWS </em><a href=\"https://aws.amazon.com/dms/\"><em>DMS</em></a><em> which works on Binlogs. Enabling binlogs adds 5â€“10% performance overhead onÂ Aurora)</em></li><li><em>Out-of-the-box support for SchemaÂ Registry</em></li><li><em>Support for Kafka 0.11â€Šâ€”â€Šexactly onceÂ delivery</em></li><li><em>Open source</em></li></ul><p><strong>Important pointers:</strong></p><ul><li>While sourcing data from Aurora, these connectors read only committed data. So, if you have any long running transactions, make sure you choose the right delay interval while sourcing this data. In our case, we have a few aggregation tables with transaction time as high as 45â€“60 mins. Note that, the max transaction time on a table will add to the overall delay in making this data available in the Data-warehouse.</li><li>While sinking the data from Kafka, make sure you configure the â€œrotate intervalâ€ connector property. This will ensure the end-to-end data delays are consistent even when there is a lean period, i.e. the number of records received are lesser than the configured batchÂ size.</li></ul><p><strong>S3 toÂ Redshift</strong></p><p>For this part of the pipeline, we needed the ability of handling duplicates, as many transactional datasets are mutable in nature. We followed the route suggested by Redshift for thisÂ process:</p><p><em>a) Load all incremental data in a stagingÂ table</em></p><p><em>b. De-dupe thatÂ data</em></p><p><em>c) Delete all corresponding entries from the mainÂ table</em></p><p><em>d) Insert newÂ records.</em></p><p>We have a multi-threaded script that incrementally loads multiple datasets, following the above process, using the Redshift Copy command for initialÂ load.</p><p>Right now, we have a 1-to-1 mapping between our transactional tables &amp; the ones in Redshift. In the long term, we plan to have denormalized view of these datasets, which makes more sense to the business &amp; all end-users. Apart from this, weâ€™re also evaluating having insert-only fact tables as our firstÂ layer.</p><p><strong>Monitoring</strong></p><p>Data pipelines are sensitive to schema changes &amp; even issues as small as source/sink network volatility can bring them down. Planning &amp; building a good monitoring system is as important as building the platform.</p><p>Schema changes are inevitable. Such scenarios can easily break our data pipeline and the dependent ETLs. Monitoring pipelines with Confluent Connectors just narrows down to playing with their standard REST APIs. Since Kafka Connect is tightly coupled with data schemas, any incompatible schema change breaks the pipeline. We have a monitoring system keeping a check on its â€œStatus APIâ€ and generating alerts whenever a â€œfailureâ€ occurs. Once the consumer processes are changed for schema compatibility, the pipeline is restarted.</p><p>Weâ€™ve come a long way, but thereâ€™s still plenty to do. Watch this space as we keepÂ sharing!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=286756496fbd\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/a-journey-to-insightful-data-286756496fbd\">A Journey to Insightful Data</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/286756496fbd",
    "categories": ["analytics", "tech", "data", "big-data", "data-warehouse"],
    "isoDate": "2018-01-17T12:38:35.000Z"
  }
]
