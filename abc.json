[
  {
    "creator": "Dream11 Engineering",
    "title": "How To Convert a Huge Frontend Monolith to a Micro-frontend",
    "link": "https://blog.dream11engineering.com/how-to-convert-a-huge-frontend-monolith-to-a-micro-frontend-d37f47697235?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 20 Apr 2021 10:27:17 GMT",
    "content:encoded": "<p>When starting a new project, most technology teams begin with a single frontend repository. This worked very well for us at Dream11 too, when our tech team was small in the early stages of projects. But as the team grew, maintaining our content management system (CMS) became more and more difficult. With a growing team, making work processes as seamless as possible to deliver high performance was a priority, so we identified problematic bottlenecks and reinvented our way of work when it came to our CMS application.</p><h3>What were the bottlenecks?</h3><p>We found that:</p><ul><li>Shared code led to too many conflicts.</li><li>Continuous integration and deployment (CI/CD) became cumbersome, overloaded, and inefficient.</li><li>Experimentation, migration, and refactor became difficult.</li></ul><p>These problems were already tackled in an architecture called microservices, vastly used in back-end architecture design. ‘Microservices’ mean dividing a monolithic application into smaller, independent services. On similar lines, Thoughtworks defines micro-frontend as <em>“An architectural style where independently deliverable frontend applications are composed into a greater whole.”</em></p><h3>What was our resolution?</h3><p>Work targets are easier to achieve when they are divided into smaller milestones. Similarly, we realized that when a product expands rapidly, it is better to divide the team into smaller verticals. Each vertical could then focus on a specific target- just like an e-commerce application, for example, where there could be separate verticals for offers, user personalization, and checkout.</p><p>We adapted a micro-frontend architecture that helped solve multiple problems. It split scary monoliths into smaller independent applications, which could then be tested, deployed, and maintained in a focused scope. This helped independent vertical teams deliver quicker without worrying about technical debts and heavy regressions, thus, amplifying the overall performance of deliveries.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qMiwXeL7YF-lgUPLZSfXxQ.png\" /></figure><p>There are multiple ways to go about implementing a micro-frontend architecture.</p><ul><li>Handle it at the infrastructure layer — Configure your infrastructure to serve multiple applications under the same domain.</li><li>Handle it at the component level — Deploy your components on delivery networks such as <a href=\"https://bit.dev/\">Bit</a>, and use them over the network or burn them into the package during build time</li><li>Handle it at the run time — You have a controller script/app running on the browser which selects which script/app to load.</li></ul><p>We identified that we could logically separate our flows easily based on routes and thus after evaluating the options, handling at the run time was the right fit for us. It enabled us to have -</p><ul><li>A micro control on switching the context of applications based on business logic.</li><li>Authentication, authorization, and layouts were implemented once.</li></ul><h3>High Level Architecture</h3><p>Relating to a microservice analogy, we had multiple independent codebases. In order to serve them as one web application, we implemented a container application that stitched together all the relevant codebases.</p><p>The container application conceptually should know when to switch applications and then bootstrap them into the current runtime. How to decide, can be based on business logic or your application. For our use case, the business flows had very less intersections, so we went with loading different applications simply based on routes.</p><p>The crucial part was bootstrapping individual applications at runtime. Every web application had an entry point where the root component resided. The container application needed access to this entry point component. The child applications made the root component available to the container by hosting it in a global context.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*RZx9GSyiC9Qrqtl2\" /></figure><p>But how did we implement the architecture? For this, we had to answer two questions for our container -</p><ul><li><strong>When to render a child application? </strong>— We grouped our applications under the router param. For example — /App-A/* would serve all routes under App-A.</li><li><strong>How to render a child application?</strong> — In all our applications, we made the assets and their URLs available using a JSON configuration, along with the entry script to execute to launch a child application.</li></ul><p>Though we used React for all our rendering needs and webpack for bundling, the concept could be applied to any other frontend frameworks.</p><h3>Low Level Design</h3><p>Now let’s get our hands dirty with some implementation details. Let’s set this up in our local systems. In the above architecture, there are two types of application:</p><ol><li><strong>Container Application</strong></li></ol><p>As mentioned earlier, this application needs to know when and how the child application loads. To achieve this, it needs -</p><ul><li>To load a specific application’s bundles based on routes</li><li>To forward asset requests to the specific child application.</li></ul><p>2. <strong>Child Applications</strong></p><ul><li>These applications are hosted independently on different domains</li><li>The child applications need to make it possible for the container to load them This involves adding scripts to mount methods on the window object on the entry point of an application.</li></ul><p>Consider that three applications were running on the following ports on the local system:</p><ol><li><a href=\"http://localhost:2000/\">http://localhost:2000</a> — Container Application</li><li><a href=\"http://localhost:2000/\">http://localhost:3000</a> — App-A</li><li><a href=\"http://localhost:2000/\">http://localhost:4000</a> — App-B</li></ol><p>In order to bootstrap a child application, the container needed to load all the assets required for the child application to start working on the browser. The information about such assets needed to be provided by each child application. Webpack, being our go-to bundler, and <a href=\"https://www.npmjs.com/package/webpack-assets-manifest\">webpack-asset-manifest</a> plugin used with webpack, produced a manifest file containing assets information as follows:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*qn6lvVjmZUxtl4yv\" /></figure><p>Let’s see how the container application structure works. The container application code comprised mainly of three files.</p><ol><li><strong>MicroFrontend.jsx</strong></li></ol><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/fcb98d86d025f824f24e35528c935a17/href\">https://medium.com/media/fcb98d86d025f824f24e35528c935a17/href</a></iframe><p>The above code is very much similar to what is recommended by <a href=\"https://camjackson.net/\">Cam Jackson</a>. As the name suggests, the purpose of this code was to load the application based on the host and <em>name </em>provided. It made an ajax request for asset-manifest.json and created a script element in the dom. Once the script was loaded, there were two functions added by the child application into its code, i.e renderAppA, and unmountAppA.</p><p>2.<strong> ContainerRoute.jsx</strong></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/71393e501a5a24e755f87111e6f3cdde/href\">https://medium.com/media/71393e501a5a24e755f87111e6f3cdde/href</a></iframe><p>The above code consists of routing information for child application. <em>/App-A/* </em>forwarded all routes to the ’s router.</p><p>3.<strong> server.js</strong></p><p>The child application may have dynamic chunking implemented in it. When a child application was served from a Container Application, all requests for chunks were to go to the same URL, but instead, they were forwarded to the child application’s URL i.e /App-A/ . To solve this problem, we found and forwarded requests to a specific server. Below is the code that was used.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/a8049bbd6678fb10ce92f60929b5d3bc/href\">https://medium.com/media/a8049bbd6678fb10ce92f60929b5d3bc/href</a></iframe><p>In this code, we forwarded all .js file requests consisting of App-A<em> </em>in it’s URL, to <em>App-A</em>.</p><p>Along with these three parts, the container app may consist of some common screens. We ended up creating a lightweight login and home page, which were shared by all child applications.</p><p>Now let’s discuss how we addressed the second part of the implementation, that is, children applications. We will take an example of one child app created with create-react-app.</p><p>Following were the changes needed in this application</p><ol><li><strong>index.jsx — attach bootstrapping functions to mount and unmount application</strong></li></ol><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/902b84814036a1dd4f41c57d74415dc4/href\">https://medium.com/media/902b84814036a1dd4f41c57d74415dc4/href</a></iframe><p>In this code, we created two functions for mounting and unmounting child applications into a container application.</p><p>2.<strong> server.js — for allowing cross-origin request based</strong></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/eeb09a4ddb1fc7454ec01b07437216ee/href\">https://medium.com/media/eeb09a4ddb1fc7454ec01b07437216ee/href</a></iframe><p>We allowed cross-domain requests coming from container applications. For security purposes, a list of domains was maintained so that other domain excluding the list could be easily stopped</p><p>That’s it! We are all set to convert any existing application into the micro frontend.</p><h3>The Pros</h3><ul><li>No tech debt was carried: Since all applications were isolated, any breaking change in one wouldn’t affect other applications</li><li>Room for experimentation: One could try different applications stacked together</li><li>Concentrated QA Effort: In the case of regression testing, there was less area to cover.</li></ul><h3>The cons</h3><ul><li>There was a slight impact on the development experience as we had to run multiple applications</li><li>There was less code sharing between applications since the stack of each application was different</li></ul><p>Overall, the benefits of micro-frontend implementation were remarkable and for us, they outweigh the drawbacks.</p><p>Written and implemented by <a href=\"https://www.linkedin.com/in/nishant-dongare-6386bb86/\">Nishant Dongre</a> and <a href=\"https://www.linkedin.com/in/yugal-bagul-22018798/\">Yugal Bagul</a></p><h3>References:</h3><ul><li><a href=\"https://martinfowler.com/articles/micro-frontends.html\">Micro Frontends</a> by <a href=\"https://camjackson.net/\">Cam Jackson</a></li><li><a href=\"https://medium.com/better-programming/5-steps-to-turn-a-random-react-application-into-a-micro-frontend-946718c147e7\">5 Steps to Turn a Random React Application Into a Micro Front-End</a> by <a href=\"https://jenniferfubook.medium.com/\">Jennifer Fu</a></li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d37f47697235\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/how-to-convert-a-huge-frontend-monolith-to-a-micro-frontend-d37f47697235\">How To Convert a Huge Frontend Monolith to a Micro-frontend</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/d37f47697235",
    "categories": ["react", "micro-frontends", "dream11"],
    "isoDate": "2021-04-20T10:27:17.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Enhancing Cloud Security With Real-Time S3 Alerts at Dream11",
    "link": "https://blog.dream11engineering.com/enhancing-cloud-security-with-real-time-s3-alerts-at-dream11-fac99079fbf4?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 19 Apr 2021 06:50:35 GMT",
    "content:encoded": "<p><strong><em>— By </em></strong><a href=\"https://www.linkedin.com/in/renin-williams-infosec/\"><strong><em>Renin Williams</em></strong></a><strong><em>, </em></strong><a href=\"https://www.linkedin.com/in/harsimran033/\"><strong><em>Harsimran</em></strong></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*U3punXGR2SU7PRUz\" /></figure><p>At Dream11, keeping our users’ data safe and secure is of primary importance. Users trust us with their information and we go above and beyond to provide the best possible online fantasy sports experience to them while they enjoy their favourite sports. Since our teams deal with a lot of user-centric and company-related data, every member of the team takes utmost effort and care to keep the data secure. Let us discuss how we made this possible with S3 alerts.</p><p><strong>The Background — Where It All Began</strong></p><p>At Dream11, we use S3 buckets. They are an integral part of our cloud computing processes and data storage which are accessed by all tech and non-tech teams with appropriate permissions. But what are S3 buckets?</p><p>S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. S3 buckets are public cloud storage resources that are similar to file folders and store objects consisting of data and its descriptive metadata. The buckets can be made either private or public, depending on the usage and requirement.</p><p><strong>The Issue</strong>:</p><p>When the access control lists (ACL) and the bucket policy are not configured properly, one can knowingly or unknowingly, land up making a bucket (and its contents) public, that is open to all on the internet. Such a misconfiguration can lead to data loss and/or breach of everything that is under the terminology of confidentiality, integrity and availability (CIA).They can go unnoticed if the necessary checks, balances, alerts and alarms are not in place. Auto-remediation can be a step further in drawing the hard-line for enhanced cloud governance.</p><p><strong>Our solution</strong>:</p><p>We believe that in order to solve any issue, it is very important to get visibility and insights into what’s happening. But how can this be achieved?</p><p>By timely detecting what, when, where, how and by whom the changes were made. In other words, by setting up event based triggers, alarms and alert for real-time notifications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JXsdFbd-WTukHn3P\" /></figure><p>As soon as someone makes a public bucket or changes the existing policies to make a bucket or its content public, based on this event, a lambda can be triggered. This lambda can scan for the change(s), captures them and sends the team a real-time alert on their communication channel(s).</p><p>The team gets the alerts and investigates to understand the situation in hand so the necessary steps may be taken.These can range from simply making the bucket private again to even purging the content.</p><p>Further, the bucket policies can be made tougher as required so the bucket owner or team may be cautioned.</p><p>Here’s how we tried different solutions:</p><ol><li><strong>We tried open source / tool-based solutions</strong></li></ol><p>We initially referred to a few open-source solutions and monitoring tools. We also used a couple of ready-made scripts from the internet to solve this issue, but it had its limitations and weren’t in line with our automation-first principle..</p><p>We then used the <strong>AWS S3 Access Analyser</strong>, a service offered by AWS. However, this was a manual trigger and had to be run every time while identifying public buckets. There was a lack of real-time and instant visibility and alerts, and so, we had to take manual human driven action, which could be missed anytime. Hence, this approach too didn’t meet our automation-first requirement.</p><p><strong>2. We developed a solution in-house</strong></p><p><strong>Our first approach:</strong></p><p>Using the python library, Boto developed by AWS, we wrote automation on top of the AWS S3 Access Analyser, to get an automated alert on Email/IM. This didn’t work as required because it relied only on the bucket itself and missed monitoring the objects in the bucket.</p><p><strong>Our second approach — EUREKA!:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*XsYTq-CZz8Kwq_iI\" /></figure><p>We followed a three-step process:</p><ul><li>Picked up S3 events from cloudtrail.</li><li>Using these Cloud trail events, we set up AWS cloudwatch rules.</li><li>These cloud watch rules in-turn triggers a custom Lambda.</li></ul><p>Our custom lambda function does <strong>a lot of conditional checks</strong> on the data received from cloudwatch and based upon the Rules defined, it triggers an alarm for any security anomaly. The alarms are set for custom notifications on our different channels.</p><p>With this solution we were able to detect:</p><ul><li>All public bucket/objects created and/or who is changing the bucket/object permissions, in real-time.</li><li>Actionable notifications were being sent to our communication channel — and on-time action was being taken.</li></ul><p>We achieved real-time alerts on our S3 buckets.</p><p>We have made many such automations that will be published soon. So, keep watching this space for more!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fac99079fbf4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/enhancing-cloud-security-with-real-time-s3-alerts-at-dream11-fac99079fbf4\">Enhancing Cloud Security With Real-Time S3 Alerts at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/fac99079fbf4",
    "categories": ["security"],
    "isoDate": "2021-04-19T06:50:35.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "To Scale In Or Scale Out? Here’s How We Scale at Dream11",
    "link": "https://blog.dream11engineering.com/to-scale-in-or-scale-out-heres-how-we-scale-at-dream11-f88ef5e71cbc?source=rss-52c59dd57d8a------2",
    "pubDate": "Fri, 02 Apr 2021 11:44:14 GMT",
    "content:encoded": "<p>— <em>By </em><a href=\"https://www.linkedin.com/in/qamar-ali-shaikh/\"><em>Qamar Ali Shaikh - CoreInfra</em></a><em> , </em><a href=\"https://www.linkedin.com/in/ritesh-sharma-a4681592/\"><em>Ritesh Sharma - SRE</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*q8aBtIDKpSda70zc\" /></figure><p>For the 100 million Dream11 users, the thrill and excitement of playing fantasy sports on our platform is unparalleled. They enjoy creating their own teams and competing with fellow fans and friends! However, from a backend perspective, there are various challenges we face in terms of variation in traffic and engagement on Dream11 majorly before the match start time. To ensure the application runs smoothly at critical times when user traffic is high, as a team, we came up with a scalable and customisable solution. And so, we were able to run multiple contests simultaneously and efficiently process millions of user requests per second without compromising their experience in playing fantasy sports.</p><p>How did we manage such variation in traffic that drastically fluctuates in short intervals on Dream11 platform ? But before we answer that, let’s have a look at the application’s architecture for better understanding.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*To5HL0bM6BGXoLqS6cBLbA.jpeg\" /><figcaption>Dream11 Architecture</figcaption></figure><h3>The architecture supports:</h3><ul><li><strong>A user base of more than a 100 Million</strong></li><li><strong>User concurrency of over 5.5 Million</strong></li><li><strong>Over a 100 Million requests per minute (RPM) at edge services</strong></li><li><strong>More than 30K+ of compute resources to support peak IPL traffic</strong></li><li><strong>More than a 100+ microservices running parallel</strong></li></ul><h3>Functional Challenges</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hUcor6wPvv-L2R7ClQC47g.png\" /><figcaption>Traffic Surge Events During Matches</figcaption></figure><p>The behavioural trend of Dream11 users is a very <strong>spiky</strong> one, (as seen in the Traffic Surge diagram above). This means that the number of users is dynamic and surges frequently, especially when they rush to create/edit fantasy teams and join contests. This also depends on multiple factors such as the popularity of the sport or match and the toss timeline (in the case of cricket matches). Such tsunamis of user traffic on the application can be divided in three main phases, namely, ‘before the match’, ‘during the match’ and ‘after the match’ (including when the winners are declared).</p><p>Once users open the Dream11 application, their <strong>journey</strong> usually oscillates between the home page, tour, team and contest pages. Hence, the load on the application shifts and accordingly, the edge layers, its dependent services and microservices have to be scaled in or out.</p><p>Interestingly, the <strong>user concurrency</strong> may rise or drop in between events or after the matches, and predicting it based on the growth of users per year, can be challenging.</p><p>Let us also consider the <strong>uncontrolled variables</strong> which generate spikes on the platform before, during and after matches. These are,</p><ul><li>User interest depends on popularity of the matches and players in real life, which affects the volume of RPM.</li><li>Match-specific events such as the toss, squad announcement, team creation by the user post squad announcement, and mid-hour events like fall of wickets, hitting of sixes, hat-tricks, breakout events and other unpredictable factors such as rain.</li></ul><p>The tsunami traffic represented by the vertical spikes in the graph above, brings tremendous volatility and load on the infrastructure.</p><h3>Challenges in Scaling</h3><h4>Auto Scaling (why it won’t work)</h4><p>Autoscaling in terms of infrastructure, has several limitations. Its <strong>provisioning time </strong>is not fast enough to support the compute requirements for users during key events! A flood of users during tsunami traffic needs short provisioning time to keep up with the spike, and it may not be suitable to have a build time and make users wait.</p><ul><li><strong>Spot availability</strong> of nodes is limited and highly competitive — especially at key hours!</li><li><strong>Step scaling</strong> may not work at this point either, as it is limited to a certain number of nodes, if Dream11’s scale is to be considered</li><li><strong>Rebalancing</strong> or rearranging the number of nodes across availability zones (AZ) based on the availability of resources, may further add provisioning cost with respect to time.</li></ul><h4><strong>Limitations of classic and application load balancers (CLB/ALB)</strong></h4><ul><li><strong>Creating load balancers shard based on throughput</strong>, as there is a limit on the number of requests generated on the load balancer. For higher throughput based on user concurrency, there is a need to create shards and manage them as per service routings.</li><li><strong>Pre-warming</strong> of ELBs must be conducted in order to handle the sudden surge of traffic.</li></ul><h4>Limitations of Cloud Control Plane (Cloud Provider)</h4><ul><li>Additionally, there are limits to the features on our cloud provider too. Application Programming Interface (API ) call rates are limited as per businesses, and this needs to be considered while allocating resources</li><li>Console operations are heavy due to the number of resources provisioned.</li><li>Operational overheads to scale out to 100+ microservices.</li></ul><h3>Solution: Predictive - Proactive Scaling</h3><h4>Our Homegrown Concurrency Prediction Model</h4><p>Our data science team at Dream11 has developed a model for predicting user concurrency using XGboost mode after trying multiple models with 100’s of features, to predict the hourly concurrency on the Dream11 platform.</p><p>We first run every match’s metadata through a linear model which gives the tier of the match. A match tier is an indicator variable for how popular the match will be.</p><p>Match tiers are then categorised (prioritised by high concurrency or those most in demand) based on their past concurrency of similar matches.</p><p>The model then iterates multiple features to predict the concurrency for the particular match. These can be features of each hour such as number of matches by tier in that hour (and in the hours around it), active users in previous hours/days, average transaction sizes etc. All this data goes through a normalisation which can take care of Dream11’s exponential growth.</p><p>To top it all, we need a suitable cost-sensitive loss function with no option for under-prediction. In all, we have over 200 variables and more data artistry than data science, making the XGBoost model work with very limited hyper parameter tuning.</p><p>As our data science team believes, <strong>“Error Analysis &gt;&gt;&gt;&gt; Hyperparameter Tuning”.</strong></p><h4>Performance Testing &amp; Gamedays</h4><p>Based on the prediction model which provides concurrency estimates, the performance team holds ‘Game Days’ to benchmark infrastructure capacity along with factoring trends based on past matches.</p><p>The performance testing framework used is <a href=\"https://about.dream11.in/blog/2020/12/finding-order-in-chaos-how-we-automated-performance-testing-with-torque\"><strong>Torque</strong></a></p><p>The infrastructure provisioning framework : <strong>Playground (Watch this space for more on this)</strong></p><p>Using Playground to provision Infrastructure and <a href=\"https://about.dream11.in/blog/2020/12/finding-order-in-chaos-how-we-automated-performance-testing-with-torque\">Torque</a> to run performance tests, the performance team certifies the following improvements for business functionality based on user concurrency predictions.</p><p><strong>Performance metrics &amp; validations :</strong></p><ul><li>Defining the application latency — the acceptable latency to serve the business</li><li>Identifying individual service capacity</li><li>Benchmarking compute and network throughput</li><li>Identifying failure and saturation points of the applications</li><li>Generating sudden spikes and surges to identify impact on backend infrastructure and identifying cascading effects in the architecture</li><li>Test Infrastructure boundaries w.r.t Compute, Network, API Limits and Operations.</li></ul><h4>Deployment optimisations to reduce provisioning time</h4><ul><li>Fully baked Amazon machine images ( AMI ) for deployments with application artefact for faster scaling</li><li>Provisioning multiple compute instance types across Availability Zones (diversified), reducing capacity challenges</li><li>Capacity Optimised allocation strategy for spot instances</li><li>Cost Optimisation ensuring 100% resources are running on spot</li><li>Notifications to failure in case of spot unavailability and enable on demand provisioning.</li><li>Weighted domain name system (DNS) records to support ELB shards.</li></ul><h4>Scaling for the key hours using Scaler.</h4><ul><li>The DevOps and SRE team at Dream11 have orchestrated a platform ‘Scaler’ which helps in ProActive scaling ,per the concurrency prediction and performance benchmarks.</li><li>Based on the performance tests with respect to the predicted concurrency and trend, the system is fed with different slabs of user concurrency and respective infrastructure to provision across microservices before the match, during the match, and after the match.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*BRQbgXcDZiD4Hfo9\" /><figcaption>Scaler Flow</figcaption></figure><p><strong>Technology and Cloud Services Used</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*n36Qctm2DQZUkQkp\" /><figcaption>Tools and Technology Used</figcaption></figure><h3>🎖Results Achieved!</h3><p>🚀 <strong>Improved quality with lower risk: Scaler</strong> helps the Dream11 DevOps/SRE team to manage Scale-In and Scale-Out operations much better, while making our process much more efficient, considering the sheer volume of matches hosted on the platform.</p><p>🚀 <strong>Faster service:</strong> It streamlined daily operational processes and infrastructural tasks, what previously took hours to complete is now achieved in minutes.</p><p>🚀 <strong>Increased flexibility: Scaler</strong> helps us save a significant amount of time, as scaling operations are now based on schedules of matches. This increases operational efficiency and enables the DevOps/SRE team to focus on making engineering improvements.</p><p>🚀 <strong>Lower infrastructure cost:</strong> As the scaling operations are scheduled based on different slabs and tiers, overall capacity of infrastructure can be provisioned based on the events of matches. This reduces the monthly infrastructure cost by 50%.</p><p>🚀 <strong>Distinctive insights: </strong>Analytics based on the scaling events and user trends provide better feedback to the machine learning model. This makes it possible to predict organic and inorganic growth patterns of infrastructure and users, which in turn helps us predict the provisioning requirements for the future.</p><h3>Future Scope of Work</h3><p>As we mature the architecture, we are looking at predictive and scheduled scaling for <strong>containers</strong> and <strong>data stores. </strong>We are also looking to optimise our infrastructure cost and to scale out realtime basis the spike we see on the Dream11 platform. To achieve this we are looking for talented engineers excited in solving infrastructure problems at scale and delivering a great product to Dream11 users.</p><p>If you think solving above challenges interests you? We at Dream11 have open positions for those who are SME’s in their domain. <a href=\"https://jobs.lever.co/dreamsports\">Apply Now</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f88ef5e71cbc\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/to-scale-in-or-scale-out-heres-how-we-scale-at-dream11-f88ef5e71cbc\">To Scale In Or Scale Out? Here’s How We Scale at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/f88ef5e71cbc",
    "categories": ["scale", "devops", "data-science", "aws", "microservices"],
    "isoDate": "2021-04-02T11:44:14.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "How does Dream11 serve millions of Personalised Contests in a match",
    "link": "https://blog.dream11engineering.com/how-does-dream11-serve-millions-of-personalised-contests-in-a-match-cbb00463b7da?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 12 Jan 2021 09:42:54 GMT",
    "content:encoded": "<p>— <em>By</em><a href=\"https://www.linkedin.com/in/srijan--gupta\"><em> Srijan Gupta</em></a>,<a href=\"https://www.linkedin.com/in/anuj3918/\"> <em>Anuj Gupta</em></a></p><p>At a time when we are collectively homebound, thanks to the pandemic, the Dream11 Indian Premier League (IPL) 2020 came like a breath of fresh air for cricket fans everywhere. Not only did the <strong>Dream11 IPL 2020</strong> fill an otherwise gaping void in the realm of sports this year, but it also kept our passion for cricket, a thread that binds us together, burning as brightly as ever.</p><p>Besides watching the matches, sports fans participated in exciting fantasy sports contests on the Dream11 app and showcased their skill and knowledge of the sport! Fans can create their own team of real-life players from upcoming matches, score points based on their on-field performance and compete with other fans. What’s more, as Title Sponsors of the Dream11 IPL, we were committed to providing fans with a secure and seamless experience.</p><figure><img alt=\"Contest sections\" src=\"https://cdn-images-1.medium.com/max/320/0*sL3arOWdYa7ckpZD\" /><figcaption>Contest sections</figcaption></figure><p>Different types of <strong>contests allow users to compete and win big, thereby increasing their engagement.</strong> Creating customised or personalised contests are a prime component for best user experience. With hundreds of highly personalised contests running on the platform simultaneously, meeting every user’s requirements, it is inevitable to encounter several challenges during the entire process. To keep these contests running smoothly, we play an entirely different ball game at the backend. So what are these, and how do we manage to address each of them?</p><h3>Business Requirements for Creating Personalized Contests for Users</h3><p>Contests are the centre of attraction for every user, which motivates us to manifest contests without any hiccups and ensure a smooth-sailing experience. For that, we need to meet a set of business requirements that will enable us to do exactly this. Each contest is different from the others in terms of variables like prize money, contest entry amount, number of winners and total participants. To pique our users’ interest and provide contest options best suited for them, the contest page is divided into Sections like ‘<em>Mega Contests,</em>’ ‘<em>Head to Head Contests</em>,’ ‘<em>Contest for Champions,</em>’ and ‘<em>Top Picks For You</em>,’ to name a few.</p><p>Several events are generated every minute as users interact with the application. We wanted to leverage the data collected from these events to create <strong>User Cohorts</strong>. A user cohort is a logical grouping of users based on their activity, playing pattern and history.</p><p>For receiving a maximum response, we apply target campaigns to compartmentalise users and serve relevant contest sections to them based on their cohorts. These cohorts keep changing in real-time basis user activity and events. <strong>Cohorts are mapped to relevant Contest Sections thus allowing users to see exclusive sections.</strong></p><p>The Product and Data team at Dream11 leverage the following features too:</p><ul><li><strong>Personalise contests based on user attributes</strong></li><li><strong>Rules based exclusive contests</strong></li><li><strong>Run A/B tests on contests and sections to figure out which variant works the best</strong></li></ul><h3><strong>How we maintain user-cohorts</strong></h3><p>Self serve admin panel is provided to write/maintain rules to create cohorts and map relevant contest sections to that cohort.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Gv9sQ-cCmgRZxWBT\" /><figcaption>User cohorts flow</figcaption></figure><p>Once the cohort is activated, a query to data-lake is fired to bootstrap users who pass the cohort rules into the cohorting engine. Then this data is streamed to kafka as <em>cohort_in</em> event to be consumed by user-cohort service.</p><p>The users keep falling in and out of the cohorts based on real-time events as being processed by the cohorting engine.</p><h3><strong>How we serve fast-filling contests at Dream11 Scale</strong></h3><p>In a typical IPL match, there is a lot of incoming traffic. Once the contests are filled to their optimum capacity, they are dynamically replaced from backend with new contests in real time to keep the inventory refreshed.</p><p>After the toss of every match, we typically see a high surge in contest joins per second. This leads to a high rate of contests creation, adding upto millions of contests in a single match.</p><p>The starting point is contest creation which is done by our AI based Contest Generation Engine (CGE). CGE calls Contest-Generation service which generates different kinds of contests in contest datastore i.e. VoltDB.</p><p>From there, we stream these generated contests to <strong>Spark Streaming</strong> where it continuously writes new contests to our contest cache store <strong>Aerospike </strong>and expires the filled contests. This needs to be done in real-time as users need to see updated contest data.</p><p>We chose Aerospike since it is a distributed, scalable database with intelligent re-balancing and data migration. But like any other distributed system it has <strong>Eventual Consistency</strong> to compensate for High Availability which leads to another important problem to be solved:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/382/0*n3tuV9ss4XdnID9A\" /><figcaption>Contests spots getting filled</figcaption></figure><p><strong><em>Hot-key scenario</em></strong><em>:</em> As so many users are playing together, our typical user concurrency is around 5 million post-toss. Since everyone is trying to play the same match, they will see a similar list of contests. Clients need to reflect the number of spots left at sub-second level. Serving 5M users from a single aerospike node is impossible as it will create a <strong>HOT-KEY</strong>. So, we need to replicate this data to other nodes and read from Master and Slaves both.</p><p><strong><em>Eventual consistency trust issue:</em></strong> If we replicate (replication factor &gt;= 10) then eventual consistency may cause users to see the ‘<em>Number of spots left’</em> increasing (different data because of eventual consistency between nodes) which can lead to big trust issues among users. The number of spots can either decrease or remain constant. Under no circumstances should they increase.</p><h3><strong>Solution:</strong></h3><p>We started doing <strong>key seeding </strong>for all active contests in aerospike while keeping the replication factor of each key as 2. Key seeding basically means creating multiple copies of the same data and writing it to cache. We configured our spark streaming to write 10 duplicate copies of <em>match_contests</em> key by appending _1, _2,…,_10 to the keys and hashed each userId so a particular user will always read from a particular copy. This made sure that a user always sees ‘<em>Number of spots left’</em> as strictly decreasing even if spark fails to write a few copies.</p><pre>int keySeed = userId % (no. of copies);<br>Map contestData = getFromAerospike(‘match_contests_’ + keySeed);</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ug_XOF5CbGd_UjAa\" /><figcaption>Active contests flow</figcaption></figure><p><strong>Request response cycle:</strong> On landing on the contests page, for each user a request is made to contest microservice which does the following:</p><ol><li>Get user’s cohorts from <em>user-cohorts</em> microservice.</li><li>Prepare a list of contest sections based on the user’s cohorts.</li><li>Read filtered contest sections from aerospike to get active contests.</li><li>Asks <em>promotions</em> microservice for any running promotions.</li><li>Sends back the response.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*RQ1_WCG9GazefJRF\" /><figcaption>Contests serving flow</figcaption></figure><p>This system has been a great success to generate contests and give a personalised touch to them through cohorting. At Dream11, we vouch for the quality and output of work to make fantasy sports an unforgettable experience for users.</p><p>Interested in building similar systems for serving 100M &amp; growing users, reach out to us <a href=\"https://jobs.lever.co/dreamsports/d3814c13-dcb8-4169-be73-7bc55ad978e7\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cbb00463b7da\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/how-does-dream11-serve-millions-of-personalised-contests-in-a-match-cbb00463b7da\">How does Dream11 serve millions of Personalised Contests in a match</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/cbb00463b7da",
    "categories": ["distributed-systems", "scale", "dream11", "sharding"],
    "isoDate": "2021-01-12T09:42:54.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Data Highway — Dream11’s Inhouse Analytics Platform — The Burden and Benefits",
    "link": "https://blog.dream11engineering.com/data-highway-dream11s-inhouse-analytics-platform-the-burden-and-benefits-90b8777d282?source=rss-52c59dd57d8a------2",
    "pubDate": "Tue, 07 Jan 2020 11:35:20 GMT",
    "content:encoded": "<h3>Data Highway — Dream11’s Inhouse Analytics Platform — The Burden and Benefits</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*Xw8WVdTzCjfYT8cfVLsv-Q.png\" /><figcaption>Scale at Dream11</figcaption></figure><p>With a whopping 40 million RPM, 2.5 million user concurrency, it is all about capturing and crunching the data of the 3 billion daily events that we receive, owing to data collection of ~4.5 TB per day.</p><p>In the current data driven world, how fair is to expect a company to count on a single data tool to fulfill business and data requirements!!?<br> I would say, it is clearly not possible. With each tool fighting to have its own unique functionality, we tend to lean towards outsourcing of different tools to satisfy our data and business needs. Fair Enough!<br> <br> However, if you see your data growing manifold every year, at some point down the line, it gets difficult for some of these tools to handle the humongous inflow of data. Our idea of Inhouse Analytics has its genesis from this fundamental need to have more control over our own data, to have more flexibility as per internal requirements and to play with the data as per our discretion, without relying much upon outsourced tools.</p><p>Don’t let this implementation give you an illusion that it is a snap! <br> The implementation of Inhouse analytics comes with its own set of advantages and challenges!<br> <br> We initially used Segment.io to capture our interaction data and Redshift to store our transactional data. Inhouse analytics involves gathering the user interaction events’ data along with transactional data at one place. This would enable us to easily map user actions to a specific transaction, revise definition of metrics as per our business, build user interaction-based audience profiles to send out a promotion, and the list goes on!</p><h3><strong>Dream11 Data Platform Architecture</strong></h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T5BXJKxZ4AWxkoaAwRCTVw.png\" /></figure><h3><strong>What does Inhouse Analytics have in store for us?</strong></h3><p><strong>#1 A Centralised Tool For Reporting</strong></p><p>With all possible data at one place, we are eventually phasing out multiple tools that we have been using to report different metrics and working towards sticking to one tool. For instance, we used Google Analytics for analysing interaction data, Segment.io to store user interaction data, Fabric to check user concurrency, Looker to build models and report transactional data.<br> <br> With Inhouse Analytics in place, we are now consuming data from Amazon Redshift/Athena/Druid and building models on Looker to report user concurrency, to track user journeys and to build interaction events based funnels.</p><p><strong>#2 User Mapping Across Multiple Platforms</strong></p><p>Currently, there are limitations with every tool that are available in the market with respect to identifying and mapping a specific user’s action across multiple platform (Web/App). Although, Google Analytics has the functionality of mapping users across platforms, it doesn’t function accurately for user journey funnels. This use case is specifically important if your company is a parent to other companies or if your business is present on both web, android and iOS.</p><p><strong>#3 Data Sampling</strong></p><p>Due to the high inflow of data, reporting tools like Google Analytics or Amplitude sample user data and show the approximate overall counts or drop offs in a funnel This might be misleading while taking product based decisions.<br> <br>Using Inhouse analytics avoids data sampling and lets us get an accurate picture of the current product state.</p><p><strong>#4 Accelerated Speed of Reporting</strong></p><p>Reporting tools take significant amount of time to load data in order to generate unsampled reports or even in case of basic reporting when the data volume is huge. This issue could be resolved when we start aggregating data as per our requirements using inhouse analytics.</p><p><strong>#5 Mapping of User Interaction and Transaction Data</strong></p><p>For businesses which involve transactions that are not invoked due to user interaction, it gets slightly difficult to analyse the impact of any interaction on the transaction. We have been using Google Analytics to map this.. However, it couldn’t solve the use case to map server events data with client events data. Inhouse Analytics would solve this problem easily as we have entire data at one place, with unique identifiers for each interaction.</p><p><strong>#6 Data Security</strong></p><p>With growing competition and data privacy issues it has become extremely important to securely store the data and avoid sharing granular data with different tool. Data security is one of the biggest advantages to have everything in house.</p><p><strong>#7 Reduce Cost</strong></p><p>With increasing volume of events, the storage and processing cost charged for each tool is shooting up. In order to limit the costs and build a sturdy system, in-house analytics seem to be the best way.</p><p><strong>#8 Data Integration with Other Sources</strong></p><p>Inhouse analytics framework gives us the flexibility to integrate with other sources by selectively sharing required data. For instance, we can build an audience in-house based on the data that we have and feed it to Clevertap to send push notifications to those users.</p><h3><strong>Implementation Challenges</strong></h3><p><strong>#1 Unique Identifier for A User</strong></p><p>We assign a unique identifier to a logged in user but what about the visitors who have no unique ID assigned to them. Google Analytics used to solve this use case by assigning a unique ID to every user (visitors/logged in) which made it easy to identify. <br> In order to solve this use case, we started mapping users based on advertiser ID/cookie ID/device ID of a user.</p><p><strong>#2 Retrospective Mapping of a Logged In User to his Non-Logged In State</strong></p><p>If a user comes to the app but is not logged in, and after some time, if he/she logs in, tools like Google Analytics/Amplitude do the retrospective mapping of logged in user to the visitor state and indicate that both are same. <br> In case of Inhouse analytics, since we get multiple entries for the same user with different IDs, it was difficult to map directly.<br> We have to work towards setting up this mapping based on device ID/advertiser ID</p><p><strong>#3 Real Time Chronological User Interaction Path</strong></p><p>This path of every user is helpful for the Customer Support team to resolve any real time queries for refunds etc.</p><p>We get realtime stream of events on Kafka. We have setup a pipeline to push this data from Kafka to Elastic search. Using Kibana dashboards, we can easily visualize the user journeys and get real-time user interaction paths to debug the issue.</p><p><strong>#4 Demographics and Location Data</strong></p><p>Google Analytics captures both demographics and user location data based on the user repository it has. However, we wouldn’t be able to track that information as there is no source to get it. This still remains a challenge and we are exploring ways to solve it.</p><p><strong>#5 Static Table Schema Not Adapting to Additional New Fields</strong></p><p>AWS Glue Crawler has helped us build a catalog and daily scheduled has ensured it has kept the catalog up-to-date for users.</p><p><strong>#6 Dynamic Funnel Creation</strong></p><p>Google Analytics provides effective dynamic funnel creation capabilities and when we moved this to Inhouse, it was fun and challenge in itself — it is an interesting journey about how we provided capabilities to build funnels dynamically. We plan to cover that in the next blog</p><p>Implemented By <a href=\"https://www.linkedin.com/in/pradip-thoke-04b4b910/\">Pradip Thoke</a>, <a href=\"https://www.linkedin.com/in/salmandhariwala/\">Salman Dhariwala</a>, <a href=\"https://www.linkedin.com/in/naincy-suman-2012/\">Naincy Suman</a>, <a href=\"https://www.linkedin.com/in/vikasrgite/\">Vikas Gite</a>, <a href=\"https://www.linkedin.com/in/lavanya-pulijala/\">Lavanya Pulijala</a>, <a href=\"https://www.linkedin.com/in/ruturajbhokre/\">Ruturaj Bhokre</a>, <a href=\"https://www.linkedin.com/in/dhanraj-gaikwad-88246723/\">Dhanraj Gaikwad</a></p><p>Written By <a href=\"https://www.linkedin.com/in/lavanya-pulijala/\">Lavanya Pulijala</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=90b8777d282\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/data-highway-dream11s-inhouse-analytics-platform-the-burden-and-benefits-90b8777d282\">Data Highway — Dream11’s Inhouse Analytics Platform — The Burden and Benefits</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/90b8777d282",
    "categories": [
      "sports-data",
      "data-engineering",
      "inhouse-analytics",
      "dream11",
      "dream11-data"
    ],
    "isoDate": "2020-01-07T11:35:20.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Building Scalable Real Time Analytics, Alerting and Anomaly Detection Architecture at Dream11",
    "link": "https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33?source=rss-52c59dd57d8a------2",
    "pubDate": "Thu, 19 Sep 2019 12:09:24 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/pradip-thoke-04b4b910/\">Pradip Thoke</a>, <a href=\"https://www.linkedin.com/in/salmandhariwala/\">Salman Dhariwala</a></p><p><strong>Introduction</strong></p><p>Building batch data analytics platform traditionally is not a new trend. While the industry is moving towards agile and shorter development cycles, scope of building data platform is no more limited to batch processing. Businesses aim for real time updates on-the-go. No one wants to know something that has broken after an hour.</p><p>Many of us must have seen an application use RDBMS OLTP directly and run SQL statements to do all these. In case you are wondering- <em>is this a good solution</em>? The answer is- it depends.</p><p>Using OLTP transactional systems to run your real time analytics might be enough for your use case. However as project requirements grow and more advanced features are needed — for example, enabling synonyms, joint analytics or doing lookups — your relational database might not be enough.</p><p>Our journey at Dream11 was no different. It started with fulfilling requirements from traditional way and as complexity started biting us in terms of volume and latency, we moved to more powerful, distributed real-time engines.</p><p><strong>Data volume at Dream11</strong></p><ul><li>Deals with 3+ TB of data per day</li><li>10M+ transactions per day</li><li>2.8 M concurrent users</li><li>Billions of clickstream events per day</li></ul><p><strong>Why Dream11 decided to implement real-time pipeline</strong></p><p>1) At the core of Dream11 Engineering, we use AWS Aurora as an OLTP system. These systems are very efficient for OLTP load but are not meant for OLAP kind of workload. We can’t perform aggregation or OLAP type of queries on these transactional systems.</p><p>2) To solve the above problem obvious solution is to run your OLAP queries on your data warehouse. In our case, it was AWS Redshift. We have ETL pipelines in place to load data from transactional system to our warehouse. These ETL pipelines run every hour thus real-time analytics is not possible on warehouse</p><p><strong>What we wanted to achieve</strong></p><p>We wanted to perform real-time analytics on the data, which is residing in the transactional system, in our case AWS Aurora database.<br>Some of the use cases are as follow: -</p><ol><li>Know the real-time rate of contest joins</li><li>Know the real-time aggregated status of payment gateways</li><li>Identify real-time anomalies eg: PG going down, unusual traffic on the system</li><li>Realtime aggregated view of outcome of marketing campaigns</li><li>How customers are using discount coupons once promotion goes live</li><li>Realtime alerting once Mega contest is above 90%</li></ol><h4>Architecture overview</h4><p>So, let’s explore How Dream11 has implemented the pipeline. To understand the architecture better, we will divide the pipeline into various stages -</p><ol><li>Pull data from various transaction systems on a common bus in real-time</li><li>Perform data enrichment</li><li>Push data into a real-time data store for a real-time analytics. This allows data visualization, rule-based anomaly detection and alerting</li><li>Operations &amp; Monitoring</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XYFledX05nJOja7kRilbdg.png\" /></figure><h4>Moving data into Apache Kafka with the Kafka Connect</h4><p>We have used Kafka connect to pull data from our transactional system to Kafka. For this, we have primarily used Debezium &amp; JDBC source.</p><p>Kafka Connect, an open source component of Kafka, is a framework for connecting Kafka with external systems such as databases, key-value stores, search indexes, and file systems. Using Kafka Connect, you can use existing connector implementations for common data sources and sinks to move data into and out of Kafka. Kafka Connect is focused on streaming data to and from Kafka, making it simpler for you to write high quality, reliable, and high-performance connector plugins. It also enables the framework to make guarantees that are difficult to achieve using other frameworks. Kafka Connect is an integral component of an ETL pipeline when combined with Kafka and a stream processing framework. Kafka Connect can run either as a standalone process for running jobs on a single machine (e.g., log collection), or as a distributed, scalable, fault tolerant service supporting an entire organisation. This allows it to scale down to development, testing, and small production deployments with a low barrier to entry and low operational overhead, and to scale up to support a large organisation’s data pipeline.</p><p><strong>Source Connector</strong><br>A source connector ingests entire databases and streams table updates to Kafka topics. It can also collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency.</p><p><strong>Sink Connector</strong><br>A sink connector delivers data from Kafka topics into secondary indexes such as Elastic search or batch systems such as Hadoop for offline analysis.</p><p><strong>The main benefits of using Kafka Connect are:</strong></p><p><strong>Data Centric Pipeline</strong> — use meaningful data abstractions to pull or push data to Kafka.<br><strong>Flexibility and Scalability</strong> — run with streaming and batch-oriented systems on a single node or scaled to an organization-wide service.<br><strong>Reusability and Extensibility</strong> — leverage existing connectors or extend them to tailor to your needs and lower time to production.</p><p><a href=\"https://github.com/debezium/debezium\"><strong>Debezium</strong></a> is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.</p><p>The<strong> JDBC source connector</strong> allows you to import data from any relational database with a JDBC driver into Apache Kafka topics. By using JDBC, this connector can support a wide variety of databases without requiring custom code for each one. Data is loaded by periodically executing a SQL query and creating an output record for each row in the result set. The database is monitored for new or deleted tables and adapts automatically. When copying data from a table, the connector can load only new or modified rows by specifying which columns should be used to detect new or modified data.</p><p>Learnings<strong>:</strong></p><ol><li>Debezium works on Binlog, it adds overhead on the database which will eventually impact the performance of the database.</li><li>JDBC source cannot detect deletes and intermediate updates.</li><li>JDBC can miss long running transactions</li></ol><h4>Enriching datasets with KSQL</h4><p>Confluent KSQL is the streaming SQL engine that enables real-time data processing against Apache Kafka It provides an easy-to-use, yet powerful interactive SQL interface for stream processing on Kafka, without the need to write code in a programming language such as Java or Python. KSQL is scalable, elastic, fault-tolerant, and it supports a wide range of streaming operations, including data filtering, transformations, aggregations, joins, windowing, and sessionization.</p><p>We have used KSQL for various use cases mainly Stream join, Data enrichment, Filtering, Aggregation</p><p>Learnings:<br> 1) Although KSQL is a very powerful tool it is in early stages. We have observed some data loss issue.<br> 2) KSQL file mode deployment is preferred over CLI mode. CLI mode some time causes duplicate records issue</p><h4>Indexing documents with Elastic Search Connector</h4><p>The Elasticsearch connector allows moving data from Apache Kafka to Elasticsearch. It writes data from a topic in Kafka to an index in Elasticsearch and all data for a topic have the same type.</p><p>Learnings:</p><ol><li>We have created weekly indexes in Elasticsearch for easy archival of older data. We can use topic index map property to create weekly indices</li><li>Supports upsert/append mode</li><li>Schema mappings needs to be handled properly</li></ol><h4>Setting up Alert rules for anomaly detections and real time dashboards</h4><p><a href=\"https://github.com/Yelp/elastalert\">ElastAlert</a> is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch</p><p>Several rule types with common monitoring paradigms are included with ElastAlert:</p><p><em>Match where there are at least X events in Y time” (frequency type)<br>Match when the rate of events increases or decreases” (spike type)<br>Match when there are less than X events in Y time” (flatline type)<br>Match when a certain field matches a blacklist/whitelist” (blacklist and whitelist type)<br>Match on any event matching a given filter” (any type)<br>Match when a field has two different values within some time” (change type)<br>Match when a never before seen term appears in a field” (new term type)<br>Match when the number of unique values for a field is above or below a threshold (cardinality type)</em></p><h4>Latency Monitoring:</h4><p>IPL is our peak time. During this time, we have observed end to end maximum latency of less than 2 minutes. New Relic monitoring dashboards in place with required alerts with thresholds in place for attention if something goes wrong with pipeline</p><h4>Wrapping it up</h4><p>With this architecture we were able to achieve ingestion rate of ~40k events/sec with an end to end latency of &lt; 50 secs. This has helped our operational team take real time decisions on adding more contests if required. Product team is using this to know current inflow, user activities and decide on strategy on giving promotions and making user journey’s more interactive. Marketing team is using this to know their campaign effectiveness in real time and make adjustments on the fly.</p><p><strong>What’s next? Data Highway</strong></p><p>We would be sharing our journey of implementing clickstream real time event processing in-house framework at large scale. That’s Data Highway. Stay tuned :)</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e20edec91d33\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/building-scalable-real-time-analytics-alerting-and-anomaly-detection-architecture-at-dream11-e20edec91d33\">Building Scalable Real Time Analytics, Alerting and Anomaly Detection Architecture at Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/e20edec91d33",
    "categories": ["dream11", "big-data", "real-time-analytics"],
    "isoDate": "2019-09-19T12:09:24.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "DATA is the New OIL: Mine it Well to Tap your Customers in Real-Time",
    "link": "https://blog.dream11engineering.com/data-is-the-new-oil-mine-it-well-to-tap-your-customers-in-real-time-e01803c321b0?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 25 Mar 2019 08:56:55 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/neha-sood-60719013/\">Neha Sood</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1022/0*S6muBLTOUrmpHfKF\" /></figure><p>Data is the new oil…only if you can reach meaningful insights out of it. Getting the most relevant insights in the fastest possible way, makes a business stand apart from the crowd. In other words, reliability and speed are the two key metrics when it comes to assessing the quality of insights. However, with growth in user base and resultant data, supporting deep analytics at a large scale becomes a challenge.</p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, our analytics use-cases range from real-time customer targeting based on their current actions, detecting behavioural anomalies, powering journey nudges basis users’ action stage, enabling the business to react without delay.</p><p>Because of our humongous scale, i.e., over 50 million registered users and 1 billion events per day, creating pertinent architecture is challenging, not to mention we are growing 4x YoY.</p><p>One such use-case was to identify fraudulent users and their transactions in near-real-time. The objective was to enable instant withdrawals for all our users. This required running a decision engine on their entire history of transactions in near-realtime, which the current architecture could not support.</p><h3>Run to the hills!</h3><p>Our transactional data currently resides in <a href=\"https://aws.amazon.com/rds/aurora/\">Aurora</a>, which could not scale for this use-case. With a microservices based architecture, all the required data is also distributed across different clusters. Therefore, running joint analytics was not possible, as well.</p><p>Below is the broad architecture to kick start the project:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*o1aDmvO41EWVGwA2\" /></figure><p>While looking for a target NoSql datastore, we focussed on the below prerequisites:</p><ul><li>Low response time for data at scale</li><li>Highly available and futuristic, in line with our growth target</li><li>Ability to handle request spikes &amp; high write volumes in peak seasons like IPL and World Cup etc.</li><li>Horizontally scalable</li></ul><p>For the above requirements, <a href=\"http://cassandra.apache.org/\">Cassandra</a> seemed a good fit. Modelling your data right (as per the use-cases) is the key to make it work. Data enrichment and joins have to be a part of the streaming layer.</p><h3>The Pipeline from Aurora (AWS RDS) to Kafka</h3><p>With our <a href=\"https://medium.com/dream11-tech-blog/a-journey-to-insightful-data-286756496fbd\">previous pipeline</a>, we already had the data flowing from Aurora to Kafka.</p><p>However, from our experience with maintaining Confluent Connectors, we knew they were not the right fit for a near-real-time use-case.</p><h4>Adios confluent connectors (why)</h4><ul><li>Long-running transactions cause data sourcing delays.</li></ul><p>For sourcing data from Aurora to Kafka through Confluent connectors, we used “Timestamp + Incrementing mode”. To ensure that a transaction with an earlier timestamp completes, before proceeding the counter, some delay needs to be added with “timestamp.delay.interval.ms”, accounting for the maximum transaction duration per table.</p><ul><li>Continuously updating records are not sourced, until their updations pause for a while.</li></ul><p>A record is sourced only when it is in the &lt;LastSourcedOffset&gt; to &lt;Current timestamp-Delay interval&gt; window, with respect to the record timestamp. If the nature of data demands continuous updates, chances are they’ll keep missing this window until the updates slow down or pause for a while.</p><p>For more details on our usage of Confluent Connectors, refer our last blog <a href=\"https://medium.com/dream11-tech-blog/a-journey-to-insightful-data-286756496fbd\"><strong><em>here</em></strong></a><strong><em>.</em></strong></p><h4><a href=\"https://www.go-fiji.com/bula.html\">Bula</a> Binlog replicators</h4><p>Thinking of CDC on transactional systems and not considering Binlog replicators is a cardinal sin. We ran a comparison across all the known and less-known pieces. Here is what we have to show:</p><p>Comparisons (as of May’18)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*27ljb2FEZ3YGo4Pj\" /></figure><p><strong>Voila!</strong></p><p>Maxwell, Streamsets and Debezium came out as the best suited for our requirements. However, with Maxwell lacking HA and Streamsets only supporting HA in its enterprise version, Debezium was the ‘chosen’ one. With a performance difference of ~15–20% under heavy load, some internal load tests comparing Streamsets community version &amp; Debezium, also favoured the latter. Being based on confluent connectors, it also supported <a href=\"https://docs.confluent.io/current/schema-registry/docs/index.html\">Schema Registry</a> out-of-the-box.</p><p><strong>Gotchas in Debezium</strong></p><ul><li>Beware, Debezium is not a scalable replicator. A single connector thrives on a single node. So you may run into performance bottlenecks. In our load tests, we started seeing lags at a consistent DML of at least 6k DMLs/sec, for more than 5 minutes. So, with consistently increasing and high load, this will be a bottleneck, although it may catch up within a couple of seconds, as soon as the load fluctuates or goes down.</li><li>With shared binlog files, across nodes, mysql master failover can get handled with a small operational overhead. All you need is to restart the Debezium connector, using their HTTP API.</li><li>Use avro data format, rather than Json to save network I/O. With Avro, we witnessed more than 50% spike in the performance.</li><li>To make Debezium suit our scale, we jumped into its open source code and tried out the below optimizations. The first one is running for us in production:</li><li>Skip deserializing delete (or any other) row operations, if they are not relevant to your streaming pipeline.</li><li>Serialize date(Time) fields as long (<a href=\"https://github.com/shyiko/mysql-binlog-connector-java/blob/master/src/main/java/com/github/shyiko/mysql/binlog/event/deserialization/EventDeserializer.java\">https://github.com/shyiko/mysql-binlog-connector-java/blob/master/src/main/java/com/github/shyiko/mysql/binlog/event/deserialization/EventDeserializer.java</a>)</li></ul><h3>Streaming: Kafka to Cassandra</h3><p>Industrial benchmarks favored <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">structured streaming</a> as a micro-batch low latency and high throughput engine. Our POC showed good results too. With easy support for late data arrivals, joining multiple streams, out-of-the-box mechanisms for fault-tolerance and state-management, this looked like the perfect fit. An icing on the cake: the code is easily unit testable.</p><p>Also, we chose EMR for running the processes.</p><h4>Why EMR?</h4><ul><li>AWS managed service</li><li>Supports resource dynamic allocation — your streaming jobs can automatically handle spikes in data volume and be able to scale up as your data grows</li><li>Auto-scaling of core and task nodes based on CloudWatch metrics</li><li>Automatic node failure handling</li></ul><h4>Gotchas in EMR</h4><ul><li>Master node stores the fsimage and the following edit logs. EMR does not merge them automatically, which results in disk full issue.</li><li>As can be concluded from the above point, EMR doesn’t have a secondary name node aka checkpoint node.</li><li>Task nodes do not have any local HDFS, unlike core nodes. So, any tasks running on these nodes will need to fetch the data from remote core nodes, for processing. This will have some performance implications.</li></ul><h3><a href=\"https://www.collinsdictionary.com/dictionary/english/beefeater\">Beefeaters</a> &amp; <a href=\"https://www.collinsdictionary.com/dictionary/english/raven\">ravens</a></h3><ul><li>With both Debezium and Spark Streaming, you can expose metrics via JMX. This helps easily integrating with centralized dashboards that can give you a fair end-to-end picture on delays and possible bottlenecks.</li><li>Make sure you also set-up alerts when binlog replicator or streaming latency exceeds the expected threshold.</li></ul><h3>Is my toast done, honey?</h3><p>What we have is, an in-house generic Streaming framework, which can read from and write to configurable Kafka and Cassandra clusters, with automatic schema mapping, using Schema Registry. With this framework, apart from the ability to join across streams, supporting dynamic filtering rules and data deduplication, one may even rename/drop/derive new fields from existing ones.</p><p>The pipeline involving multi-stream join, with large join and watermarking windows in the streams, takes max 2 sec end-to-end. While the single stream pipelines, with some filtering rules, column add/drop/rename, works on a delay of less than 100 ms.</p><p>We have just begun our journey of real-time deep analytics. The next frontier is identifying and alerting anomalies and tracking near-real-time user behaviour patterns.</p><p><strong>Follow us in our journey and keep learning with us.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e01803c321b0\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/data-is-the-new-oil-mine-it-well-to-tap-your-customers-in-real-time-e01803c321b0\">DATA is the New OIL: Mine it Well to Tap your Customers in Real-Time</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/e01803c321b0",
    "categories": ["big-data", "dream11"],
    "isoDate": "2019-03-25T08:56:55.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "ElasticSearch @Dream11",
    "link": "https://blog.dream11engineering.com/elasticsearch-dream11-30328d913cd5?source=rss-52c59dd57d8a------2",
    "pubDate": "Mon, 25 Feb 2019 15:55:53 GMT",
    "content:encoded": "<p><strong>ElasticSearch @</strong><a href=\"https://www.dream11.com/\"><strong>Dream11</strong></a></p><p>Scale to serve over 500,000+ events per second, 5 TB+ ingestion capacity and provide logging, search for micro services, security analytics along with anomaly detection</p><p>By <a href=\"https://www.linkedin.com/in/sanjeev-jaiswal-3ab9259/\">Sanjeev Jaiswal</a>, <a href=\"https://www.linkedin.com/in/astrobounce\">Arvind Sharma</a></p><p><em>How DevOps team at </em><a href=\"https://www.dream11.com/\"><em>Dream11</em></a><em> built ‘Elasticsearch as a Service’ for our microservices , security, data services logging and analytics needs in the face of high events frequency and data ingestion requirements</em></p><p><strong>Introduction:</strong></p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, multiple microservices and applications are using Elasticsearch as a core infrastructure engine to solve problems like centralized logging, search capability in applications, storing security related WAF logs and anomaly detection etc. All these use cases involve querying/searching, structured as well as unstructured data with specific keywords at very high events per second with large amounts of logs ingestion per hour/day.</p><p><strong>Why Elasticsearch is in Obvious Choice?:</strong></p><p><a href=\"https://www.dream11.com/\">Dream11</a> was using a SAAS based solution for its centralized logging needs.</p><p>Rsyslog was forwarding the logs to the SAAS based provider for our services. However, during our peak game cycle amid popular sporting events, the logging service failed to deliver in time. Events per second -EPS was so high that, the logs to the SAAS provider lagged behind and did not show up in dashboards even after days or weeks. This defeated the very purpose of having the need for real-time logging system as a service. Engineering, DevOps and QA teams were severely hampered and not able to debug issues in time and had to revert to additional monitoring tools or actually having to login to systems for debugging. The SAAS provider acknowledged that due to our scale, they would need to re-architect their solution. This could take couple of precious months! Hence we decided to build in-house Elastic search for our peak scale, data ingestion and internal usecases. This was the rise of ‘ElasticSearch as a Service’.</p><p>AWS Elastic search was ruled out after initial analysis due to our high throughput events per second rate(EPS rate) of 200K/second and logging rate of 1 TB to 1.5 TB per day during peak gaming season and matches.</p><p><strong>ElasticSearch and FluentD evaluation:</strong></p><p>Before implementing the solution, we evaluated different log forwarders and aggregators Log stash, Rsyslog and Fluentd. We appreciated Fluentd’s integration, performance, support and plugins as compared to others.Therefore, we decided to go ahead with an internal ‘Elastic FluentD Kibana’ stack(s) which can be managed within a ‘reasonable’ time and DevOps resources.</p><p>Terraform and Ansible were used in building the ElasticSearch stack as a service.</p><p><strong>Centralised Logging using EFK</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*pchfwm1Oye-fV7PV\" /></figure><p><strong>Architectural components :</strong> EFK stack at <a href=\"https://www.dream11.com/\">Dream11</a> is divided into the following components</p><ul><li><strong>Cluster Discovery using discovery-ec2 plugin — </strong>Cluster discovery of Elasticsearch is done using ec2 discovery plugin and unique tags of the cluster.</li><li><strong>Log Forwarders ( td-agent )</strong> — td-agent is installed on each and every server and is just used for log shipping to log aggregators</li><li><strong>Log Aggregators ( td-agent ) </strong>— Log aggregators process the stream of data coming from log forwarders based on different tags . Data is enriched and sent to Elasticsearch coordinator nodes for indexing .</li><li><strong>Elasticsearch coordinators and Kibana </strong>— Elasticsearch coordinator nodes are Elasticsearch node types which act as cluster load balancer. These also have Kibana nodes installed, Kibana just takes single Elasticsearch URL till 6.4.0. This HA of Kibana is maintained using HA of coordinator nodes.</li><li><strong>Elasticsearch masters </strong>— These nodes are cluster managers whose work is just to manage cluster</li><li><strong>Elasticsearch Hot Nodes — </strong>These are Elasticsearch data nodes with node attribute property set to hot. These nodes have more CPU and RAM nvme SSD for higher write throughput .</li><li><strong>Elasticsearch Warm Nodes — </strong>These are again Elasticsearch data nodes with node attribute property set to warm. These nodes stores all indices from previous day to retention period .</li><li><strong>AWS S3 — </strong>S3 is used for archival of indices . All indices are kept as per our index retention policies on hot and warm nodes.When the retention period expires they are moved to S3 and deleted from the cluster.</li><li><strong>Curator — </strong>Curator is used for keeping indices retention policies , archival , movement of index from to warm , increasing replica count.</li><li><strong>Elastalert Kibana Plugin: </strong>Since we are using open source version of Elasticsearch, alerting on Elasticsearch data doesn’t comes out of box . We have used another open source tool, i.e Elastalert developed by Yelp and its Kibana UI plugin developed by Bitsensor to provide efficient alerting framework .</li></ul><p><strong>Hardware Configurations: </strong>Indexing is resource intensive.So choosing hardware at our scale has been quite cumbersome. There had been a lot of load test to finally come to these hardware configuration and numbers .</p><ul><li><strong>Master Nodes</strong>: <strong>3 m5.xlarge</strong> instances used as masters are the one which holds all metadata information and perform all cluster management tasks . To avoid split brain scenario odd number of master is used along with ec2 discovery plugin.<br> discovery.zen.minimum_master_nodes:2</li><li><strong>Hot and Warm Nodes: 5 i3.2xlarge </strong>instances are used as hot nodes with node attributes set to hot or warm depending on the node type . i3.2xlarge provides high end machines with 1.9 TB nvme disk which outruns the performance of normal EBS backed SSD disks. <strong>10 d2.xlarge</strong> instance are used as warm nodes for index storage upto retention period. d2 instances have 3 instance disks with 2TB throughput HDD eac .</li><li><strong>Client/Coordinator &amp; Kibana Nodes: </strong>These are in ASG’s with <strong>r5.2xlarge </strong>instances which scale according to workload.Each machine is has master, data , ingest set to false to use it as coordinator nodes. Moreover, every machine is having Kibana installed to get rid of SPOF of Kibana .</li><li><strong>Fluentd Aggregators: 5 c5.2xlarge </strong>are used for aggregators instances which perform enrichment and transformation.</li></ul><p><strong>Index Lifecycle Management : </strong>For managing lifecycle of indices curator framework is used . Curator is another tool written in python which can be used for replica management, snapshot and restore etc. Here a sample action file for curator is used for index management</p><p>actions:</p><p>1:</p><p>action: allocation</p><p>description: “Apply shard allocation filtering rules to the specified indices”</p><p>options:</p><p>key: box_type</p><p>value: warm</p><p>allocation_type: require</p><p>wait_for_completion: false</p><p>timeout_override:</p><p>continue_if_exception: false</p><p>disable_action: false</p><p>filters:</p><p>- filtertype: pattern</p><p>kind: prefix</p><p>value: logstash-</p><p>- filtertype: age</p><p>source: name</p><p>direction: older</p><p>timestring: ‘%Y.%m.%d’</p><p>unit: days</p><p>unit_count: 1</p><p>2:</p><p>action: delete_indices</p><p>description: “Deleted selected indices”</p><p>options:</p><p>timeout_override: 300</p><p>continue_if_exception: false</p><p>ignore_empty_list: True</p><p>filters:</p><p>- filtertype: pattern</p><p>kind: prefix</p><p>value: logstash-</p><p>- filtertype: age</p><p>source: name</p><p>direction: older</p><p>timestring: ‘%Y.%m.%d’</p><p>unit: days</p><p>unit_count: 7</p><p><strong>Security Analytics Using EFK</strong></p><p>Another use case where we are highly dependent on EFK is performing security analytics . We use AWS WAF to secure our websites. To make more out of AWS WAF, analysing WAF logs is a goldmine . AWS WAF can stream WAF logs in realtime using Amazon Kinesis Firehose. Firehose has certain limitation for the sinks (storage of logs).It currently provides only 4 options which are</p><ul><li>AWS Elasticsearch Service</li><li>AWS S3</li><li>Redshift</li><li>Splunk</li></ul><p>As we built our own high efficient highly scalable Elasticsearch cluster, we did some POC to push these logs to our Elasticsearch cluster . For this we utilized AWS S3 and it’s event notification service which pushes metadata event in SQS for every new data put into s3 bucket. Using another Fluentd plugin we created consumers of SQS whose capability was to read from SQS metadata and then finally capture data from s3 bucket and push it to our Elasticsearch cluster .</p><p>Below architecture diagram describes the complete flow for same.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*VJFi6Ou0nlWfafsr\" /></figure><p><strong>KPI Monitoring using Elasticsearch and Kibana</strong></p><p>Another use case for which we are using Elasticsearch is KPI monitoring where KPI’s are being pushed into Kafka. We are using KSQL and Kafka to transform the data. Kafka Connect Elasticsearch connector pushes this data into Elasticsearch.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Ps1djHSfbh0cWLWO\" /></figure><p><strong>Results:</strong></p><p>The entire stack(s) were load-tested and tweaks were made constantly to achieve stability and performance. We finally managed to achieve our peak scale requirements and ingestion rate after few weeks of tweaking and testing.We now use Terraform scripts to setup the Elasticsearch stack(s) for any new services or requirements on demand as a ‘service’, much better than what AWS or any other SAAS provider can provide in terms of cost, stability, performance and resources.</p><p><strong><em>Cost savings has been a huge side effect because we manage our ElasticSearch service with the desired high performance, throughput along with customizations that can be done quickly with minimal resources!</em></strong></p><p><strong>Adoption of our stack company wide:</strong></p><ol><li>Since the ‘ElasticSearch as a Service’ proved successful, we used it for our Security Analytics needs as well for analysis of WAF logs. Custom queries were written to scan suspicious IP addresses and block them after review on demand. A separate stack was setup for the security analytics and alerting was implemented using ‘ElastAlert’ for alerting our Security team</li><li>Our stack has then been extended for use for Anomaly detection and Contest fill rate by our DataScience Engineering team. Separate Kibana dashboards and visualisations have been tuned for different requirements and performance.</li></ol><p><strong>Metrics:</strong></p><p>Estimated ingestion capacity of our ElasticSearch stack(s) was measured at approx 5 TB — 6TB per day, which means capacity to hold approx 150TB to 180 TB data in 30 days! Events per second have been recorded at 500,000 events per second per day maximum as of now and can be scaled further if needed.</p><p><strong>Further optimisations and future plans:</strong></p><ol><li>Implementation of Federated ElasticSearch services, so that failure of one EFK stack for a specific use-case does not affect other users of that EFK stack.</li><li>Introducing Kafka as messaging and buffer service. During peak traffic, we have seen that FluentD aggregators can get backlogged and buffers filled very quickly. We plan to introduce Kafka in the architecture while pumping data to FluentD aggregators to absorb any temporary back-pressure problems and store the data on disk. With introduction of Kafka as buffer approach, data can be forwarded at a later point in time to EFK stack without any data loss.</li><li>Authentication and authorization services using SearchGuard.</li><li>Machine Learning module to analyse logs and train models on our security data for automatic alerting on suspicious trends and activities from our application logs and WAF.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30328d913cd5\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/elasticsearch-dream11-30328d913cd5\">ElasticSearch @Dream11</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/30328d913cd5",
    "categories": ["elasticsearch"],
    "isoDate": "2019-02-25T15:55:53.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "Dream11 Android Application Architecture",
    "link": "https://blog.dream11engineering.com/dream11-android-application-architecture-303f04b6b5b2?source=rss-52c59dd57d8a------2",
    "pubDate": "Wed, 09 Jan 2019 11:04:14 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/rahul-panchal-2410a034/\">Rahul Panchal</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*H6RBEqzLAAdbFh_UXfpTuw.png\" /></figure><p><strong>Designing the mobile application architecture</strong></p><p>Over the years, mobile application developers have experimented with various standard architecture patterns like Model View Controller (MVC), Model View Presenter (MVP), Model View ViewModel (MVVM), and clean architecture et al. These patterns need improvisations to implement it for specific requirements of the mobile app. While designing the architecture, the first step is to identify and state the objectives. Below were the objectives identified by us:</p><ol><li>Single source of truth.</li><li>The business logic should be suitable for unit testing and reuse.</li><li>Follow <a href=\"https://hackernoon.com/solid-principles-simple-and-easy-explanation-f57d86c47a7f\">SOLID</a> principles to ensure scalability, flexibility and integration of new components in the future. Such scalability and flexibility should come only at a nominal overhead cost.</li><li>It should adapt to the necessary platform APIs and external SDKs (like Facebook, analytics SDKs etc.) without compromising the fundamentals.</li><li>Each and every class of the project must be in-line with the architectural principles. This helps in seamless initialization and any further reference to the class.</li></ol><p>In this blog series, we will discuss the architecture of <a href=\"https://www.dream11.com/\">Dream11</a> Android app, and future improvements that we plan to undertake:</p><p><a href=\"https://www.dream11.com/\"><strong>Dream11</strong></a><strong> Android Architecture</strong></p><p>Our philosophy is to design and maintain a <a href=\"http://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">Clean Architecture</a>. We have segregated it in 4 layers, View, Presenter, Model and Services. We are using RxJava for data streaming, Dependency Injection for object accessing and DataBinding library to update the view and obtain events from the view. It is important to note that we are using data binding, but not the MVVM philosophy, where View Models are the most intelligent entities. For us, View models are just POJO classes which hold data for a view and obtain events from view.</p><p>Clean Architecture has guidelines for the object dependency (producer-consumer) and the code structure</p><ol><li><strong>Producer-Consumer object dependency <br></strong>Having a clean<strong> </strong>architecture means that the producer should not be dependent on the consumer, as in below diagram, where view layer is consuming the observables from the presenter and the presenter is consuming the observables from model layer and so on. In this case, producer layer objects can be created without the dependency on the consumer layer</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*MVvI9-yRWwKv1fNM\" /></figure><p><strong>2. Code structure </strong>We can segregate mobile application code in 4 parts.</p><ol><li>Platform specific code (Service Layer, View Layer)</li><li>Enterprise specific code (Feature Layer)</li><li>Application specific code (Feature Layer)</li><li>Interface/Adapters (Presenter Layer)</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/854/0*36jmtgKzTPX7cFfk\" /></figure><p>We have written enterprise and application specific logic in the same layer to reduce communication between layers and centralise all decision making at one place.</p><p><strong>View Layer: </strong>It comprises of Activities and Fragments. Each flow has one Activity and multiple Fragments for respective screens. For example, <strong>LoginActivity</strong> manages different screens (fragments) of login flow. This approach provides tremendous flexibility and ease. For instance, whenever we need to discontinue the flow, we can simply close the activity. We also use flow specific activities to trigger prerequisite execution.</p><p><strong>Presenter Layer: </strong>It is an interface/adapter between view layer and the model layer. It has below responsibilities:-</p><ol><li>It creates ViewModels using the data received from the feature layer and exports them to view layer through observables</li><li>It delegates the events of view layer to respective feature layer class</li><li>While mapping models to view models, it also adds view specific logic, if required</li></ol><p><strong>Feature Layer</strong></p><p>This is the most intelligent layer in our architecture. It contains the enterprise logic and the app logic (deciding the flow of an application). Below are some interesting characteristics of this layer:</p><ul><li><strong>Feature classes are segregated on the basis of business logic types: </strong>Since data sharing across different app components needs wiring code, we minimized the number of feature classes by using logic type and not screens like <strong>LoginFeature</strong>, <strong>PaymentFeature</strong>. As a result, the logic for all login related screens are handled by <strong>Login Feature</strong>. Similarly, the logic for all payment related screens is handled by <strong>PaymentFeature</strong> class, because same business components can share the data with relative ease.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/267/0*RYhtJXH3L5QwDrka\" /></figure><p>Feature classes can communicate with each other through Request Message Pool, RMP is a special kind of event bus, we will discuss it in detail in the next blog</p><ul><li><strong>Own/Monitor the data:</strong> We have a single source of truth for data. But, instead of the single state container, it is fragmented in respective feature classes. For instance, the <strong>PaymentFeature</strong> class takes care of the Payment state of the app, so all the feature classes are a singleton, created using the Factory-Pattern.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/593/0*PrfPmnLGEPZdAx1e\" /></figure><ul><li><strong>Communication:</strong> There is an input and two output flows on the feature layer. One who initiates the action and gets the output as the return of method flow <strong>Observable&lt;FeatureResponse&gt;</strong>. Since our feature classes are also maintaining a single source of truth of data, with every change of data, they push it on <strong>FeatureUpdate</strong> channel. This is subscribed by all the other related views.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/284/0*UhEORgNRV0rxonxm\" /></figure><p><strong>Service Layer</strong></p><p>Model Layer should be platform independent. This allows all the platform specific functions like making HTTP calls, a message read permission, log in through Facebook and Google, saving or retrieving values from database to be written at the service layer.</p><p>Some Android SDK APIs seeks message read permissions while the others (Facebook, etc.) need an instance of the activity. Clean architecture principles do not permit access to the platform object instance in enterprise or app specific code. Additionally, we cannot put them in the view layer because a producer cannot depend on its consumer objects.</p><p>To solve this problem, we created <strong>UIListener</strong> interface. Here, the object is being passed to the service layer class, which provides current activity and application object to service classes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/739/0*oQAD1n8-lpzkuB9e\" /></figure><p><strong>Things to be improved</strong>:</p><ol><li><strong>Java to Kotlin</strong>: As of now, we are using Java. But we have also started Kotlin for some components and features. Data classes of Kotlin are very effective for Layered Architecture since every layer has to ensure that its original data objects cannot be changed by the consumer layers. For this, we need to create a copy of data objects while passing it to consumer layer.</li><li><strong>Removing side effects and separating code in pure and impure functions</strong>: As of now, we are separating code in pure and impure functions manually. But in the future, we would like to create a framework which enforces pure functions and separates impure logic in an impure function. This will make it easy to write test cases.</li><li><strong>REST to Graphql: </strong>As an organisation, we are progressing from REST to Graphql, as it allows enhanced communication between client and server. It has certain advantages like being strictly typed and the best place to write common logic for all the clients. It reduces the complexity of code at the client side, reduces state size and provides view ready data to them. Since it is an abstraction layer between client and server, their architectures can be developed more independently.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=303f04b6b5b2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/dream11-android-application-architecture-303f04b6b5b2\">Dream11 Android Application Architecture</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/303f04b6b5b2",
    "categories": [
      "android",
      "android-app-development",
      "mobile-app-development",
      "mobile-apps",
      "clean-architecture"
    ],
    "isoDate": "2019-01-09T11:04:14.000Z"
  },
  {
    "creator": "Dream11 Engineering",
    "title": "A Journey to Insightful Data",
    "link": "https://blog.dream11engineering.com/a-journey-to-insightful-data-286756496fbd?source=rss-52c59dd57d8a------2",
    "pubDate": "Wed, 17 Jan 2018 12:38:35 GMT",
    "content:encoded": "<p>By <a href=\"https://www.linkedin.com/in/neha-sood-60719013/\">Neha Sood</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*MSlkshBJKjXD3rj7Q4F48Q.jpeg\" /></figure><blockquote>Despite the sense of promise surrounding Big Data, most companies estimate they are only analysing 12% of their data, while 80% of data on average is unstructured and does not lend itself to analysis easily.</blockquote><p>Big Data is much more than simply a matter of size — it presents an opportunity to discover key insights and emerging trends in data, makes businesses more agile, board room decisions better informed, and answer questions that have previously been considered unanswerable. With all the hype around big data, insightful data is eventually most important to business.</p><p>At <a href=\"https://www.dream11.com/\">Dream11</a>, with a growing user base of over 18 million users &amp; ~3x YoY growth, running deep analytics &amp; building useful insights from the data, started seeing scalability bottlenecks. The need for having a central &amp; scalable data platform became unavoidable. <em>This post is about how we overcame the scalability and architecture challenges.</em></p><p><strong>Challenges faced:</strong></p><p>1.<strong> Evolving/Changing Needs</strong></p><p>The fast pace at which we were growing and the rising deep analytics requirements soon outdated our existing systems. Amazon <a href=\"https://aws.amazon.com/rds/aurora\">Aurora</a> is our choice of backend relational database and all our transactional data resides there. With a rapidly growing, current data volume of a few TBs, running deep data analytics was not possible anymore.</p><p>2. <strong>Fragmented Analytics</strong></p><p>All our transactional data &amp; user-activities event data is being maintained in 2 distinct platforms. While one is in-house, the other one is a store maintained by a third party tool, Segment. With 2 disconnected data platforms, there is no way to run joint analytics. For instance: we need to run a simple A/B test on our platform wherein we want to experiment with the colour of “Join Contest” button. The success metric is how many of our users end up doing a transaction after they click on the particular button. This needs access to transactional data and clickstream data that are on different and disjointed data platforms.</p><p>3. <strong>Real-time analytics at scale</strong></p><p>The current data pipeline lacked streamlining. There was a huge opportunity here to enable &amp; deliver real-time analytics.</p><p><strong>The execution:</strong></p><p><strong>Step 1: Identifying the Data Lake</strong></p><p>To begin with, we needed to identify a <strong>Data Lake</strong>, where we could keep all type of data from all possible sources, in its raw and native format. This unstructured/semi-structured/structured data could then be refined and sourced into other data-stores based on the use-cases as well as business requirements.</p><p>Primary features we needed from the Data Lake:</p><p><em>▪ High Durability, scalability, availability &amp; data security at low cost</em></p><p><em>▪ Support for heterogeneous data types: structured, unstructured &amp; streaming</em></p><p><em>▪ Easy to use: Data ingestion to analytics</em></p><p>We narrowed down our choice to <a href=\"http://aws.amazon.com/s3\"><strong>S3</strong></a>.</p><p><strong>Why S3 as Data Lake?</strong></p><p><em>▪ 11 9s of Durability; 4 9s of availability</em></p><p><em>▪ High performance, low cost &amp; low maintenance solution</em></p><p><em>▪ Decouple storage &amp; compute. Both can be scaled independently</em></p><p><em>▪ Polyglot Analytics: Provision most suitable processing engine/system based on the business requirement</em></p><p>This is the broad platform overview we came up with:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*TfzIkeXhoRG555kSGLg_iA.png\" /></figure><p><strong>Step 2: Identifying the Data Warehouse</strong></p><p>Next, we needed a data-warehouse that would store hot/warm data, with at least these features</p><p><em>▪ Easy to scale &amp; highly performant</em></p><p><em>▪ Supports the existing BI tools we have already invested-in (monetarily &amp; “effort-wise”)</em></p><p><em>▪ Easy maintenance</em></p><p><a href=\"https://aws.amazon.com/documentation/redshift/\">Redshift</a> worked for us perfectly. Five key performance enablers for Redshift: <a href=\"https://en.wikipedia.org/wiki/Massively_parallel\">MPP</a> engine, <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html\">columnar data storage</a>, <a href=\"https://aws.amazon.com/blogs/aws/data-compression-improvements-in-amazon-redshift/\">data compression</a>, <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/c-query-planning.html\">query optimisation, and compiled code</a>. Columnar storage optimises query costs by increasing the effective storage capacity of the data nodes and improves performance by reducing I/O needed to process SQL requests. Redshift’s execution engine also transforms each query into compiled code, which then runs in parallel on each compute node.</p><p>Some other side benefits included:</p><ul><li><em>Its SQL</em></li><li><em>Jump-Start — you can set up a robust and fast data warehouse with no DBA in a few hours.</em></li><li><em>Already on AWS — Since our application stack is already on AWS, working with Redshift didn’t involve any big leaps</em></li><li><em>Automatic updates &amp; patching</em></li><li><em>Less maintenance overheads</em></li></ul><p>An important benefit of our choices of Data Lake and Data Warehouse was that they worked really well with each other. With S3, data ingestion in Redshift can be massively parallelised, with each compute node directly working on data load, rather than being bottlenecked on leader node’s network I/O.</p><p>This is the Data Platform we have in place right now:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*luxRpFmur_oSJICM3uzr0w.png\" /></figure><p>While the user-events data still needs to be handled and pipelined into the system, the transactional data becomes available in the Data Warehouse within 2 hours of occurrence.</p><p>Some important pointers:</p><ol><li>Using Kafka as the communication pipeline gives this architecture additional ability to also do real-time analytics on this data, if/as needed.</li><li>For every structured dataset pushed to Kafka, we also maintain corresponding metadata in the “Schema Registry”, for easy consumption and also for handling schema changes.</li><li>In S3, each dataset is placed into difference keyspaces, further partitioned by year-month-date-hour, for data pruning optimizations and easy incremental loads. Data-partitioning rule and granularity is also configurable per data source, if needed.</li></ol><p><strong>Step 3: Identifying data pipeline building blocks</strong></p><p><strong>Aurora to S3: Confluent connectors</strong></p><p>Confluent is a platform that improves Apache Kafka by expanding its integration capabilities, adding tools to optimise and manage Kafka clusters. Confluent Platform makes Kafka easier to build and easier to operate. For sourcing data from our Aurora cluster to S3 via Kafka, we are using <a href=\"https://www.confluent.io/product/connectors/\">Confluent Connectors</a>.</p><p>Why Confluent Connector:</p><ul><li><em>Easy to scale &amp; maintain, with transparent execution</em></li><li><em>Supports Avro</em></li><li><em>Support for JDBC/Kafka/S3</em></li><li><em>No back-pressure on source DB (unlike AWS </em><a href=\"https://aws.amazon.com/dms/\"><em>DMS</em></a><em> which works on Binlogs. Enabling binlogs adds 5–10% performance overhead on Aurora)</em></li><li><em>Out-of-the-box support for Schema Registry</em></li><li><em>Support for Kafka 0.11 — exactly once delivery</em></li><li><em>Open source</em></li></ul><p><strong>Important pointers:</strong></p><ul><li>While sourcing data from Aurora, these connectors read only committed data. So, if you have any long running transactions, make sure you choose the right delay interval while sourcing this data. In our case, we have a few aggregation tables with transaction time as high as 45–60 mins. Note that, the max transaction time on a table will add to the overall delay in making this data available in the Data-warehouse.</li><li>While sinking the data from Kafka, make sure you configure the “rotate interval” connector property. This will ensure the end-to-end data delays are consistent even when there is a lean period, i.e. the number of records received are lesser than the configured batch size.</li></ul><p><strong>S3 to Redshift</strong></p><p>For this part of the pipeline, we needed the ability of handling duplicates, as many transactional datasets are mutable in nature. We followed the route suggested by Redshift for this process:</p><p><em>a) Load all incremental data in a staging table</em></p><p><em>b. De-dupe that data</em></p><p><em>c) Delete all corresponding entries from the main table</em></p><p><em>d) Insert new records.</em></p><p>We have a multi-threaded script that incrementally loads multiple datasets, following the above process, using the Redshift Copy command for initial load.</p><p>Right now, we have a 1-to-1 mapping between our transactional tables &amp; the ones in Redshift. In the long term, we plan to have denormalized view of these datasets, which makes more sense to the business &amp; all end-users. Apart from this, we’re also evaluating having insert-only fact tables as our first layer.</p><p><strong>Monitoring</strong></p><p>Data pipelines are sensitive to schema changes &amp; even issues as small as source/sink network volatility can bring them down. Planning &amp; building a good monitoring system is as important as building the platform.</p><p>Schema changes are inevitable. Such scenarios can easily break our data pipeline and the dependent ETLs. Monitoring pipelines with Confluent Connectors just narrows down to playing with their standard REST APIs. Since Kafka Connect is tightly coupled with data schemas, any incompatible schema change breaks the pipeline. We have a monitoring system keeping a check on its “Status API” and generating alerts whenever a “failure” occurs. Once the consumer processes are changed for schema compatibility, the pipeline is restarted.</p><p>We’ve come a long way, but there’s still plenty to do. Watch this space as we keep sharing!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=286756496fbd\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://blog.dream11engineering.com/a-journey-to-insightful-data-286756496fbd\">A Journey to Insightful Data</a> was originally published in <a href=\"https://blog.dream11engineering.com\">Dream11 Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "dc:creator": "Dream11 Engineering",
    "guid": "https://medium.com/p/286756496fbd",
    "categories": ["analytics", "tech", "data", "big-data", "data-warehouse"],
    "isoDate": "2018-01-17T12:38:35.000Z"
  }
]
